{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seasonal-shark",
   "metadata": {},
   "source": [
    "# vectorizing job offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-shanghai",
   "metadata": {},
   "source": [
    "## aim\n",
    "- cluster job offeres by similarity based on a dictionary of skills\n",
    "\n",
    "## outline\n",
    "- preprocess doc2vec with full job offers\n",
    "- train model\n",
    "- test similarity of job descriptions\n",
    "- cluster offers (Kmeans, KNN)\n",
    "\n",
    "## outcome\n",
    "unicorns in a meadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seeing-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import joblib\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import math\n",
    "import multiprocessing\n",
    "import gensim.models.doc2vec\n",
    "import time\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "behavioral-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('../../../raw_data/processed_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "future-ranking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7859, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "familiar-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag_language'] = df['tag_language'].fillna(value='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "herbal-heater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_text</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_info</th>\n",
       "      <th>query_text</th>\n",
       "      <th>source</th>\n",
       "      <th>job_link</th>\n",
       "      <th>tag_language</th>\n",
       "      <th>reviews</th>\n",
       "      <th>job_info_tokenized</th>\n",
       "      <th>job_text_tokenized</th>\n",
       "      <th>job_text_tokenized_titlecase</th>\n",
       "      <th>job_title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Junior) Data Engineer (f/m/x)</td>\n",
       "      <td>Customlytics ist die führende App Marketing Be...</td>\n",
       "      <td>Customlytics GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>(Junior) Data Engineer (f/m/x)\\nCustomlytics G...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[junior, data, engineer, fmx, customlytics, gm...</td>\n",
       "      <td>[customlytics, ist, die, führende, app, market...</td>\n",
       "      <td>[Customlytics, ist, die, führende, App, Market...</td>\n",
       "      <td>[junior, data, engineer, fmx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Responsibilities\\n\\nAs working student (m/f/x)...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin]</td>\n",
       "      <td>[responsibilities, as, working, student, mfx, ...</td>\n",
       "      <td>[Responsibilities, As, working, student, mfx, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin, teilzeit, pr...</td>\n",
       "      <td>[aufgaben, als, werkstudent, mwd, it, arbeites...</td>\n",
       "      <td>[Aufgaben, Als, Werkstudent, mwd, IT, arbeites...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        job_title  \\\n",
       "0  (Junior) Data Engineer (f/m/x)   \n",
       "1                                   \n",
       "2                                   \n",
       "\n",
       "                                            job_text            company  \\\n",
       "0  Customlytics ist die führende App Marketing Be...  Customlytics GmbH   \n",
       "1  Responsibilities\\n\\nAs working student (m/f/x)...         Aroundhome   \n",
       "2  Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...         Aroundhome   \n",
       "\n",
       "  location                                           job_info    query_text  \\\n",
       "0   Berlin  (Junior) Data Engineer (f/m/x)\\nCustomlytics G...  data science   \n",
       "1   Berlin                   Aroundhome6 Bewertungen - Berlin  data science   \n",
       "2   Berlin  Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...  data science   \n",
       "\n",
       "        source job_link tag_language  reviews  \\\n",
       "0  scrape_json      NaN           en      NaN   \n",
       "1  scrape_json      NaN           en      NaN   \n",
       "2  scrape_json      NaN           de      NaN   \n",
       "\n",
       "                                  job_info_tokenized  \\\n",
       "0  [junior, data, engineer, fmx, customlytics, gm...   \n",
       "1                  [aroundhome, bewertungen, berlin]   \n",
       "2  [aroundhome, bewertungen, berlin, teilzeit, pr...   \n",
       "\n",
       "                                  job_text_tokenized  \\\n",
       "0  [customlytics, ist, die, führende, app, market...   \n",
       "1  [responsibilities, as, working, student, mfx, ...   \n",
       "2  [aufgaben, als, werkstudent, mwd, it, arbeites...   \n",
       "\n",
       "                        job_text_tokenized_titlecase  \\\n",
       "0  [Customlytics, ist, die, führende, App, Market...   \n",
       "1  [Responsibilities, As, working, student, mfx, ...   \n",
       "2  [Aufgaben, Als, Werkstudent, mwd, IT, arbeites...   \n",
       "\n",
       "             job_title_tokenized  \n",
       "0  [junior, data, engineer, fmx]  \n",
       "1                             []  \n",
       "2                             []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collect-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select english jobs\n",
    "df_eng = df.copy()\n",
    "df_eng = df_eng[df_eng['tag_language'] == 'en']\n",
    "df_eng.reset_index(inplace=True)\n",
    "df_eng.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "american-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_text</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_info</th>\n",
       "      <th>query_text</th>\n",
       "      <th>source</th>\n",
       "      <th>job_link</th>\n",
       "      <th>tag_language</th>\n",
       "      <th>reviews</th>\n",
       "      <th>job_info_tokenized</th>\n",
       "      <th>job_text_tokenized</th>\n",
       "      <th>job_text_tokenized_titlecase</th>\n",
       "      <th>job_title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Junior) Data Engineer (f/m/x)</td>\n",
       "      <td>Customlytics ist die führende App Marketing Be...</td>\n",
       "      <td>Customlytics GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>(Junior) Data Engineer (f/m/x)\\nCustomlytics G...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[junior, data, engineer, fmx, customlytics, gm...</td>\n",
       "      <td>[customlytics, ist, die, führende, app, market...</td>\n",
       "      <td>[Customlytics, ist, die, führende, App, Market...</td>\n",
       "      <td>[junior, data, engineer, fmx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Responsibilities\\n\\nAs working student (m/f/x)...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin]</td>\n",
       "      <td>[responsibilities, as, working, student, mfx, ...</td>\n",
       "      <td>[Responsibilities, As, working, student, mfx, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full Stack Developer (m/f/d)</td>\n",
       "      <td>We’re Phiture: a leading mobile growth consult...</td>\n",
       "      <td>Phiture</td>\n",
       "      <td>BerlinKreuzberg</td>\n",
       "      <td>Full Stack Developer (m/f/d)\\nPhiture - Berlin...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[full, stack, developer, mfd, phiture, berlink...</td>\n",
       "      <td>[were, phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[Were, Phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[full, stack, developer, mfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>We are 18,000+ employees strong, operating in ...</td>\n",
       "      <td>PRA Health Sciences</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>PRA Health Sciences - Berlin</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[pra, health, sciences, berlin]</td>\n",
       "      <td>[we, are, employees, strong, operating, in, mo...</td>\n",
       "      <td>[We, are, employees, strong, operating, in, mo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Head of Finance</td>\n",
       "      <td>Head of Finance (m/f/d)\\nAt Home our mission i...</td>\n",
       "      <td>Home HT GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Head of Finance\\nHome HT GmbH2 Bewertungen - B...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[head, of, finance, home, ht, gmbh, bewertunge...</td>\n",
       "      <td>[head, of, finance, mfd, at, home, our, missio...</td>\n",
       "      <td>[Head, of, Finance, mfd, At, Home, our, missio...</td>\n",
       "      <td>[head, of, finance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        job_title  \\\n",
       "0  (Junior) Data Engineer (f/m/x)   \n",
       "1                                   \n",
       "2    Full Stack Developer (m/f/d)   \n",
       "3                                   \n",
       "4                 Head of Finance   \n",
       "\n",
       "                                            job_text              company  \\\n",
       "0  Customlytics ist die führende App Marketing Be...    Customlytics GmbH   \n",
       "1  Responsibilities\\n\\nAs working student (m/f/x)...           Aroundhome   \n",
       "2  We’re Phiture: a leading mobile growth consult...              Phiture   \n",
       "3  We are 18,000+ employees strong, operating in ...  PRA Health Sciences   \n",
       "4  Head of Finance (m/f/d)\\nAt Home our mission i...         Home HT GmbH   \n",
       "\n",
       "          location                                           job_info  \\\n",
       "0           Berlin  (Junior) Data Engineer (f/m/x)\\nCustomlytics G...   \n",
       "1           Berlin                   Aroundhome6 Bewertungen - Berlin   \n",
       "2  BerlinKreuzberg  Full Stack Developer (m/f/d)\\nPhiture - Berlin...   \n",
       "3           Berlin                       PRA Health Sciences - Berlin   \n",
       "4           Berlin  Head of Finance\\nHome HT GmbH2 Bewertungen - B...   \n",
       "\n",
       "     query_text       source job_link tag_language  reviews  \\\n",
       "0  data science  scrape_json      NaN           en      NaN   \n",
       "1  data science  scrape_json      NaN           en      NaN   \n",
       "2  data science  scrape_json      NaN           en      NaN   \n",
       "3  data science  scrape_json      NaN           en      NaN   \n",
       "4  data science  scrape_json      NaN           en      NaN   \n",
       "\n",
       "                                  job_info_tokenized  \\\n",
       "0  [junior, data, engineer, fmx, customlytics, gm...   \n",
       "1                  [aroundhome, bewertungen, berlin]   \n",
       "2  [full, stack, developer, mfd, phiture, berlink...   \n",
       "3                    [pra, health, sciences, berlin]   \n",
       "4  [head, of, finance, home, ht, gmbh, bewertunge...   \n",
       "\n",
       "                                  job_text_tokenized  \\\n",
       "0  [customlytics, ist, die, führende, app, market...   \n",
       "1  [responsibilities, as, working, student, mfx, ...   \n",
       "2  [were, phiture, a, leading, mobile, growth, co...   \n",
       "3  [we, are, employees, strong, operating, in, mo...   \n",
       "4  [head, of, finance, mfd, at, home, our, missio...   \n",
       "\n",
       "                        job_text_tokenized_titlecase  \\\n",
       "0  [Customlytics, ist, die, führende, App, Market...   \n",
       "1  [Responsibilities, As, working, student, mfx, ...   \n",
       "2  [Were, Phiture, a, leading, mobile, growth, co...   \n",
       "3  [We, are, employees, strong, operating, in, mo...   \n",
       "4  [Head, of, Finance, mfd, At, Home, our, missio...   \n",
       "\n",
       "             job_title_tokenized  \n",
       "0  [junior, data, engineer, fmx]  \n",
       "1                             []  \n",
       "2  [full, stack, developer, mfd]  \n",
       "3                             []  \n",
       "4            [head, of, finance]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "radio-frost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7547, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "signed-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings\n",
    "def join_strings(text):\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strategic-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "def lemmatize_words(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "settled-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    return text\n",
    "\n",
    "#['heute', 'weiter', 'zur', 'bewerbung', 'diesen', 'job', 'melden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "important-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "df_eng['clean'] = df_eng['job_text_tokenized'].apply(join_strings).apply(lemmatize_words)\\\n",
    "    .apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-patent",
   "metadata": {},
   "source": [
    "## model doc2vec 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-determination",
   "metadata": {},
   "source": [
    "Conclusions :)\n",
    "- ~ 700 offers - 100 epocs\n",
    "    - model performs ok, but tends to cluster according to company\n",
    "    - texts with very high similarity (> 0.90) are likely to be duplicated job adds\n",
    "    - looks like the model first shows offers based on duplicates, then company, then position (probably because of semantics)\n",
    "\n",
    "\n",
    "- 2500 offers - 150 epocs\n",
    "    - still clusters by company\n",
    "    - add more data? or try bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "duplicate-testament",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-231f3d794934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts_tagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtexts_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtexts_tagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# tag texts\n",
    "texts = df_eng['clean']\n",
    "\n",
    "def tag_text(texts):\n",
    "    texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "    return texts_tagged\n",
    "\n",
    "texts_tagged = tag_texts(texts)\n",
    "texts_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "breeding-edmonton",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_tagged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-34f0f66f9d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reduced dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtexts_tagged_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts_tagged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtexts_tagged_small\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts_tagged' is not defined"
     ]
    }
   ],
   "source": [
    "# reduced dataset\n",
    "texts_tagged_small = texts_tagged[:3000]\n",
    "texts_tagged_small[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "criminal-parker",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_tagged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c7bd3abdc06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_to_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts_tagged\u001b[0m \u001b[0;31m# texts_tagged_small, texts_tagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# build vocabulary with CBOW (dm=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_dbow = Doc2Vec(documents=data_to_train,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts_tagged' is not defined"
     ]
    }
   ],
   "source": [
    "data_to_train = texts_tagged_small # texts_tagged_small, texts_tagged\n",
    "\n",
    "# build vocabulary with CBOW (dm=0)\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_dbow = Doc2Vec(documents=data_to_train,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(data_to_train), \n",
    "                     min_count=1,\n",
    "                     workers=cores)\n",
    "\n",
    "# train the model\n",
    "model_dbow.train(data_to_train, total_examples=model_dbow.corpus_count, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-retrieval",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dbow.save('../../../models/doc2vec_3000_15_epochs')\n",
    "#joblib.dump(model_dbow, filename='../../../models/doc2vec_3000_20_epochs.joblib' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-johns",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dbow.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-desert",
   "metadata": {},
   "source": [
    "### test the model by hand and with copy-pasted text\n",
    "\n",
    "**test model with texts in the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "muslim-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_loaded = Doc2Vec.load('../../../models/doc2vec_all_10_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continent-verification",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def similar_jobs(tokenized_job, offers):\n",
    "    ''' input: tokenized job offers, number of offers \n",
    "        returns tags of top x most similar job offers and similarity probabilities\n",
    "    '''\n",
    "\n",
    "    # infer vector from text \n",
    "    infer_vector = model_loaded.infer_vector(tokenized_job)\n",
    "    # find similar offers\n",
    "    similar_documents = model_loaded.docvecs.most_similar([infer_vector], topn = offers)\n",
    "\n",
    "    return similar_documents\n",
    "\n",
    "\n",
    "def print_top_jobs(text, offers=5):\n",
    "    \n",
    "    \"\"\" input: index of text in dataframe and number of offers we want to see\n",
    "        prints text of the offers\n",
    "    \"\"\"\n",
    "    \n",
    "    tags = similar_jobs(text, offers)\n",
    "    indices = [int(tag[0].replace('tag_', '')) for tag in tags]  \n",
    "    \n",
    "    #print(text)\n",
    "    print(f\"{tags}\\n\")\n",
    "    for num in indices:\n",
    "        print(f\"{df_eng['job_title'][num], df_eng['company'][num], df_eng['job_text'][num]}\\n {filtered_texts[num]} -------------END------------\\n \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "violent-costa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_tagged_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c8355d7d0380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_tagged_small\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'texts_tagged_small' is not defined"
     ]
    }
   ],
   "source": [
    "similar_jobs(texts_tagged_small[0][0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-development",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-thing",
   "metadata": {},
   "source": [
    "## test model with copy-pasted job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hybrid-progressive",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## change case to lower\n",
    "import string\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "## remove numbers from the corpus\n",
    "def remove_number(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \n",
    "    return text\n",
    "\n",
    "## remove special puncutation from text\n",
    "def remove_punctuation(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interracial-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer = \"\"\"\n",
    "The Data Science team at OLX Group is responsible for building algorithmic solutions to facilitate transactions between buyers and sellers. We are developing personalization technologies and optimization strategies that have a direct impact on OLX’s users as well as the company's bottom line.\n",
    "\n",
    "You will be encouraged to research state-of-the-art machine learning, in the areas of user segmentation, image metadata extraction (including multi-label classification and tagging with deep learning), semi-supervised learning, recommender systems, and more. Applying these methods to core OLX product platforms deployed in the cloud that are affecting the user experience for millions of visitors per month, rolling your solutions to production, analysing model results offline and online, and measuring site impact.\n",
    "\n",
    "What you will be doing:\n",
    "\n",
    "Work in multi-functional teams with people from different backgrounds\n",
    "Find opportunities where data science will make an impact\n",
    "Help to translate business requirements into machine learning models\n",
    "Build effective solutions with machine learning\n",
    "Bring machine learning services to productions together with engineers\n",
    "Measure the impact of your models on company goals\n",
    "Collaborate with internal and external stakeholders\n",
    "\n",
    "\n",
    "Who we’re looking for someone who has:\n",
    "\n",
    "Strong analytical and software development background\n",
    "At least 2 to 3 years of professional data science experience or equivalent time in PhD studies.\n",
    "Experience with at least one of the following machine learning frameworks: Scikit-Learn, TensorFlow, PyTorch, (or similar)\n",
    "Hands-on experience in SQL\n",
    "Strong engineering background: good knowledge of Python and good understanding of best engineering practices\n",
    "Proficient in English with excellent written and oral communication skills\n",
    "Position based in Poznan or Warsaw\n",
    "\n",
    "\n",
    "Nice to have\n",
    "\n",
    "Experience bringing models in production and serving models at scale\n",
    "Experience using AWS for deploying machine learning solutions\n",
    "Experience with building data pipelines using tools like Spark and Airflow\n",
    "Exposure to other programming languages such as Kotlin, Java, Scala, etc\n",
    "Exposure to production infrastructure and DevOps practices: monitoring, alerting, CI/CD, container-orchestrating platforms, and infrastructure-as-code tools (Grafana, Prometheus, Kubernetes, Terraform)\n",
    "\n",
    "What we’ll give you:\n",
    "\n",
    "Competitive compensation and benefits\n",
    "Contributing to the global OLX Group\n",
    "A passionate and diverse team of data scientists spanning several tech hubs across the globe.\n",
    "The opportunity to learn from each other and become better every day\n",
    "Competitive salary and good benefits\n",
    "Company Mobile phone, laptop of your choice: Laptop MacBook Pro, Windows or Linux, Notebook, PC, any tool you might need\n",
    "\n",
    "\n",
    "What you need to know about us:\n",
    "\n",
    "OLX is the world’s leading classifieds platform in high-growth markets. It’s available in more than 40 countries and in over 50 languages. The platform makes it so easy to connect people to buy, sell or exchange used goods and services.\n",
    "OLX is part of the OLX Group, a global product and tech company with 19 brands, +40 countries, +5000 people and one mindset.\n",
    "Our mission is to make it super easy for people to buy and sell almost anything, boosting local economies.\n",
    "We are proud to be different, and we work differently too. We combine the spirit and agility of a start-up with the maturity that comes from being part of a 100 year-old company.\n",
    "We are curious, ambitious and allergic to corporate interference. We improvise, experiment and push each other further, embracing uncertainty and driving change.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "corporate-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer = \"\"\"\n",
    "\n",
    "TD Reply is an innovation and marketing consultancy and part of the Reply Group. We take a data-driven and execution-oriented approach to drive organizational change through meaningful insights. In our Berlin and Beijing offices we are around 90 thinkers, developers, analysts, designers, consultants, visualizers, futurologists and organizers. We’ve helped transform the business of global leading brands such as Coca-Cola, BMW, adidas, Miele, FrieslandCampina, Volkswagen, Lufthansa, Postbank, Deutsche Bahn, L’Oreal, and Telefónica.\n",
    "\n",
    "You & us\n",
    "\n",
    "We drive innovation in marketing and data science and believe that magic happens when data meets imagination. We generate value by applying this mix to real-world business problems for the world’s leading companies and brands. Together with our consultants, developers and designers we help to build impactful products and drive digital transformation for our clients. Our solutions predict the future of consumer behavior, find the best media budget allocation, guide brand management operations and to provide insights and management solutions on how to drive business. If you are keen to shape the future of data driven management solutions and break down boundaries in marketing and data, you are welcome in our data engineering team. At TD Reply, you will play a central role in shaping product design, technical decision-making and delivering value to the company and our clients.\n",
    "\n",
    "Your tasks will include\n",
    "\n",
    "Implementing and maintaining production-level, robust data analytics scripts in Python or R\n",
    "Build and shape scalable technical infrastructure, testing and monitoring\n",
    "Define and put into action development, quality assurance, and support processes\n",
    "Keep code quality high through pair programming, code reviews, code analytics\n",
    "Crunching and describing data sets containing important consumer/customer insights on structured and unstructured data\n",
    "Integrate publicly available datasources, external APIs, databases and leverage our reporting platform Pulse\n",
    "Promote the spirit of Data Engineering throughout the company\n",
    "\n",
    "There is a match if…\n",
    "\n",
    "… your profile is completed by demonstrated skills in a relevant programming language, e.g. R or Python, for a minimum of 2 years\n",
    "\n",
    "… you have a solid understanding of coding techniques, robust code, and error handling in data pipelines\n",
    "\n",
    "… you have worked with DBMS like MongoDB, Athena/Presto, Elasticsearch or Snowflake\n",
    "\n",
    "… you have worked with Cloud Services like AWS, Azure or GCP\n",
    "\n",
    "… you have experience in connecting with external RESTful APIs\n",
    "\n",
    "…. you possess a deep understanding of data modelling and transformation, and have handled large datasets in the past\n",
    "\n",
    "…. you hold a degree in IT/Engineering/Econometrics/Mathematics/Statistics with at least 2 years’ working experience\n",
    "\n",
    "… you like working in an agile environment and being responsible for the technical design, implementation, maintenance, monitoring and automated testing of software\n",
    "\n",
    "… you are a team player who likes to share, discuss and work on ideas / tasks with your colleagues\n",
    "\n",
    "\n",
    "That’s in it for you\n",
    "\n",
    "A startup atmosphere in a sustainably successful company located in the center of Berlin – modern office with rooftop terrace and stunning views included\n",
    "Flexible, family-friendly working hours\n",
    "Work with a young, creative and diverse team\n",
    "Get an opportunity to try out new technologies\n",
    "A chance to actively shape products and projects\n",
    "Projects & clients that will make you feel proud to work for\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "wicked-glory",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_offer = to_lower(offer)\n",
    "token_offer = remove_number(token_offer)\n",
    "token_offer = remove_punctuation(token_offer)\n",
    "token_offer = lemmatize_words(token_offer)\n",
    "token_offer = remove_stopwords(token_offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "tropical-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01941206, -0.02264883,  0.08563899, ..., -0.04149463,\n",
       "       -0.02167387, -0.07774137], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_vector = model_loaded.infer_vector(token_offer)\n",
    "infer_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "assumed-jones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tag_157', 0.9139636158943176),\n",
       " ('tag_413', 0.6613494753837585),\n",
       " ('tag_353', 0.6545976996421814),\n",
       " ('tag_371', 0.6463046669960022),\n",
       " ('tag_327', 0.6298779845237732)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_documents = model_loaded.docvecs.most_similar([infer_vector], topn = 5)\n",
    "similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "happy-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_index = [text[0].replace('tag_', '') for text in similar_documents]\n",
    "# top_offers = pd.DataFrame(df_eng.iloc[top_index]['job_text'])\n",
    "# top_offers['similarities'] = [text[1] for text in similar_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "similar-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "grateful-hometown",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tag_157', 0.9139636158943176], ['tag_413', 0.6613494753837585], ['tag_353', 0.6545976996421814], ['tag_371', 0.6463046669960022], ['tag_327', 0.6298779845237732]]\n",
      "\n",
      "('DATA ENGINEER', 'TD Reply GmbH', 'That’s us\\nTD Reply is an innovation and marketing consultancy and part of the Reply Group. We take a data-driven and execution-oriented approach to drive organizational change through meaningful insights. In our Berlin and Beijing offices we are around 90 thinkers, developers, analysts, designers, consultants, visualizers, futurologists and organizers. We’ve helped transform the business of global leading brands such as Coca-Cola, BMW, adidas, Miele, FrieslandCampina, Volkswagen, Lufthansa, Postbank, Deutsche Bahn, L’Oreal, and Telefónica.\\nYou & us\\nWe drive innovation in marketing and data science and believe that magic happens when data meets imagination. We generate value by applying this mix to real-world business problems for the world’s leading companies and brands. Together with our consultants, developers and designers we help to build impactful products and drive digital transformation for our clients. Our solutions predict the future of consumer behavior, find the best media budget allocation, guide brand management operations and to provide insights and management solutions on how to drive business. If you are keen to shape the future of data driven management solutions and break down boundaries in marketing and data, you are welcome in our data engineering team. At TD Reply, you will play a central role in shaping product design, technical decision-making and delivering value to the company and our clients.\\nYour tasks will include\\nImplementing and maintaining production-level, robust data analytics scripts in Python or R\\nBuild and shape scalable technical infrastructure, testing and monitoring\\nDefine and put into action development, quality assurance, and support processes\\nKeep code quality high through pair programming, code reviews, code analytics\\nCrunching and describing data sets containing important consumer/customer insights on structured and unstructured data\\nIntegrate publicly available datasources, external APIs, databases and leverage our reporting platform Pulse\\nPromote the spirit of Data Engineering throughout the company\\n\\nThere is a match if…\\n… your profile is completed by demonstrated skills in a relevant programming language, e.g. R or Python, for a minimum of 2 years\\n… you have a solid understanding of coding techniques, robust code, and error handling in data pipelines\\n… you have worked with DBMS like MongoDB, Athena/Presto, Elasticsearch or Snowflake\\n… you have worked with Cloud Services like AWS, Azure or GCP\\n… you have experience in connecting with external RESTful APIs\\n…. you possess a deep understanding of data modelling and transformation, and have handled large datasets in the past\\n…. you hold a degree in IT/Engineering/Econometrics/Mathematics/Statistics with at least 2 years’ working experience\\n… you like working in an agile environment and being responsible for the technical design, implementation, maintenance, monitoring and automated testing of software\\n… you are a team player who likes to share, discuss and work on ideas / tasks with your colleagues\\n\\nThat’s in it for you\\nA startup atmosphere in a sustainably successful company located in the center of Berlin – modern office with rooftop terrace and stunning views included\\nFlexible, family-friendly working hours\\nWork with a young, creative and diverse team\\nGet an opportunity to try out new technologies\\nA chance to actively shape products and projects\\nProjects & clients that will make you feel proud to work for\\n\\nInterested?\\nPlease submit your resume to our REPLY Recruiter here.\\nvor 6 Tagen\\n- Weiter zur Bewerbung\\nDiesen Job melden')     \n",
      "-------------END------------\n",
      " \n",
      "('Senior Data Engineer (f/m/x) in Berlin / Remote', 'TechMiners GmbH', \"The Opportunity\\nEver been frustrated with business and tech not understanding each other? We believe this friction can be minimized, using data insights.\\nOur solutions allow non-tech people like CEOs, investors and other business decision makers to make choices that normally require deep tech understanding. The “magic” behind this is our software product turning unique data insights into actionable advice, transparency and ultimately trust; independently of the stakeholders' actual understanding of technology.\\nAs a Senior Data Engineer, you will be responsible for ingesting, maintaining and analyzing data from many sources, supporting development, consultants and other colleagues alike. Your passion for understanding and explaining technology will shape our data culture.\\n\\u200d\\nYour Role\\nOwn the data processing architecture that powers our solutions by building, maintaining and optimizing\\nData infrastructure, internal tools and processes for ingesting, cleaning, processing and transforming data from various sources\\nData quality testing strategies & processes\\nDocumentation and communication of all the above\\nContinuously research and optimize our data tech stack\\nSupport our consultants in generating insights from complex data, defining and performing analyses of specific companies’ data setups\\nSet up and maintain the BI platform together with the BI team\\nContribute to a strong data culture that lives up to TechMiners vision\\n\\u200d\\nYour Skills & Experience\\n3-5 years experience as a Data Engineer in larger setups with strong data culture and complex scenarios\\nStructured, organized and self managed working style\\n\\u200d\\nDemonstrable, strong experience in\\nstructured research and selection of technologies (we still have choices to make)\\nsetup and maintenance of DWH, especially data lakes and analytics within AWS, GCP or Azure (certificates are nice-to-have)\\nextraction, processing and transformation of data from various sources\\ncreation, maintenance and deployment of testable data processing pipelines\\nvisualization of data and creation of dashboards\\ncoding in Python, Julia or other relevant languages\\n\\u200d\\nPassion to support, coach and mentor your colleagues on all things data\\nFluent English in speaking and writing is a must. Other languages are nice-to-have\\n\\u200d\\nYour Benefits\\nSee many different tech companies from the inside, quickly building unique experience in how tech organizations really work (a skill normally acquired during years in leading tech positions)\\nMake work life in tech companies more enjoyable and productive by enabling better decisions on technology through our solutions\\nCompany culture built on trust, transparency & ownership. Focus on outcome, not on output\\nRegular feedback sessions and a clear career path\\nFlexible hours around core office times and a family friendly culture. We are driven by excellent results and expect everybody to deliver, leaving you in charge of the “how”\\nQuarterly Team Retreats & Friday chillouts\\nLearning & Development Budget of 2.000€ p.a.\\nLatest, performant hardware & tools you need to excel\\nComfortable loft office in the heart of Berlin Prenzlauer Berg / Mitte\\nWe encourage remote work, but strongly believe in the power of smart people together in one room. We also really enjoy in-person time with amazing colleagues\\n\\u200d\\nYour Team\\nOur data-mining & analytics platform is inspired by data-driven approaches to engineering. What started as an idea to explore advantages over “manual” tech audits, today enables our specialized advisory services to leading investors, unique insights and speedy delivery of true expertise.\\nTechMiners’ success is possible thanks to our team of A-Players, founded by 3 battle-proven entrepreneurs with diverse experience in technology, company building, investing and strategy consulting.\\nWe constantly iterate to achieve a most productive environment for brilliant minds looking for a serious challenge, truly meaningful work and amazing colleagues. If you share our belief that an integrated, mutual understanding of Business and Tech is key to outstanding organizations and excellent results, we would like to hear from you.\\nYour Turn: apply now!\\nwebentwickler-jobs\\nVor mehr als 30 Tagen\\n- Weiter zur Bewerbung\\nDiesen Job melden\")     \n",
      "-------------END------------\n",
      " \n",
      "('Senior Data Engineer (f/m/x)', 'TechMiners', 'The Opportunity\\nEver been frustrated with business and tech not understanding each other? We believe this friction can be minimized, using data insights.\\nTechMiners’ solutions enable both tech and non-tech roles (like CEOs and investors) to make decisions that normally require deep technical skills and lengthy analysis. Our unique software product turns insights from tech data into actionable advice, transparency and ultimately trust; independently of the stakeholders’ level of technology understanding.\\nAs a Senior Data Engineer, you will be responsible for ingesting, maintaining and analyzing data from many sources, supporting development, consultants and other colleagues alike. Your passion for understanding and explaining technology will shape our data culture.\\nYour Role\\nOwn the data processing architecture that powers our solutions by building, maintaining and optimizing\\nData infrastructure, internal tools and processes for ingesting, cleaning, processing and transforming data from various sources\\nData quality testing strategies & processes\\nDocumentation and communication of all the above\\nContinuously research and optimize our data tech stack\\nSupport our consultants in generating insights from complex data, defining and performing analyses of specific companies’ data setups\\nSet up and maintain the BI platform together with the BI team\\nContribute to a strong data culture that lives up to TechMiners vision\\nYour Skills & Experience\\n3-5 years experience as a Data Engineer in larger setups with strong data culture and complex scenarios\\nStructured, organized and self managed working style\\nDemonstrable, strong experience in:\\nstructured research and selection of technologies (we still have choices to make)\\nsetup and maintenance of DWH, especially data lakes and analytics within AWS, GCP or Azure (certificates are nice-to-have)\\nextraction, processing and transformation of data from various sources\\ncreation, maintenance and deployment of testable data processing pipelines\\nvisualization of data and creation of dashboards\\ncoding in Python, Julia or other relevant languages\\nPassion to support, coach and mentor your colleagues on all data related topics\\nFluent English in speaking and writing is a must. Other languages are nice-to-have\\nYour Benefits\\nSee many different tech companies from the inside, quickly building unique experience in how tech organizations really work (a skill normally acquired during years in leading tech positions)\\nMake work life in tech companies more enjoyable and productive by enabling better decisions on technology through our solutions\\nCompany culture built on trust, transparency & ownership. Focus on outcome, not on output\\nRegular feedback sessions and a clear career path\\nFlexible hours around core office times and a family friendly culture. We are driven by excellent results and expect everybody to deliver, leaving you in charge of the “how”\\nQuarterly Team Retreats & Friday chillouts\\nLearning & Development Budget of 2.000€ p.a.\\nLatest, performant hardware & tools you need to excel\\nComfortable loft office in the heart of Berlin Prenzlauer Berg / Mitte\\nWe encourage remote work, but strongly believe in the power of smart people together in one room. We also really enjoy in-person time with amazing colleagues\\u200d\\nYour Team\\nOur data-mining & analytics platform is inspired by data-driven approaches to engineering. What started as an idea to explore advantages over “manual” tech audits, today enables our specialized advisory services to leading investors, unique insights and speedy delivery of true expertise.\\nTechMiners’ success is possible thanks to our team of A-Players, founded by 3 battle-proven entrepreneurs with diverse experience in technology, company building, investing and strategy consulting.\\nWe constantly iterate to achieve a most productive environment for brilliant minds looking for a serious challenge, truly meaningful work and amazing colleagues. If you share our belief that an integrated, mutual understanding of Business and Tech is key to outstanding organizations and excellent results, we would like to hear from you.\\nYour Turn: apply now!\\nBerlinStartupJobs.com\\nvor 12 Tagen\\n- Weiter zur Bewerbung\\nDiesen Job melden')     \n",
      "-------------END------------\n",
      " \n",
      "('Data Engineer (m/w/d)', 'zolar', 'Deine Aufgaben\\n\\nYou design, create and manage scalable ETL/ELT pipelines for a growing number of data sources from different teams at zolar\\nYou improve and maintain our Data Warehouse and Data Lake solutions making use of best practices (“green-field” project)\\nYou enhance data quality by keeping the data clean and accurate with the data source systems\\nYou work on exciting new projects that can range from new data pipelines to building complex tools for our teams\\nYou work closely with the Business Intelligence and the IT team\\nSo passt du zu uns\\n\\nYou have experience ( >3 years) with managing different Data Warehousing solutions, data pipelines, and data modelling\\nYou have a bachelor’s degree in a data driven study program such as Informatics, Information Sciences, Engineering, Computer Science, or related discipline\\nYou have proficient experience in at least one programming language (e.g. Python) and SQL\\nExperience with Cloud services like AWS Lambda, ECS and API-Gateways is a plus\\nYou like working in a team and actively support your colleagues\\nYou like taking on responsibility for projects and can see them through until the end\\nYou are willing to continuously learn more in the world of business intelligence\\nYou are fluent in English\\nWas zolar dir bietet\\n\\nPioneer of the energy revolution to achieve a 100% renewable energies supply\\nBe part of a team that is motivated to make an impact each day\\nFlexible working hours and contract of employment of indefinite duration\\nThe possibility to work partly remote\\nPlenty of room for own ideas and creative implementation of innovative solutions\\nOur open feedback culture, we offer regular feedback discussions\\nNumbers and goals are transparently communicated during our monthly All-Stars meeting\\nReduced Urban Sports Club M-membership\\nFree energy suppliers such as fruits and drinks\\nOur company culture, which is important to us - team events happen regularly\\nDeine Ansprechpartnerin\\n\\nAnna Heidrich is looking forward to your application!\\nOur Karriereseitegives you a first impression of our company culture.\\nvor 21 Tagen\\n- Weiter zur Bewerbung\\nDiesen Job melden')     \n",
      "-------------END------------\n",
      " \n",
      "('Senior Data Engineer (m/w/t)', 'Quandoo GmbH', 'As a member of the Data Engineering Team, you will be responsible for:\\ndevelopment and upkeep of data ingestions and processing pipelines for data used by analysts in other teams\\nmonitoring and maintenance of data quality and reliability\\nworking with business analysts and bring results of their work under your management\\nsupport analysts with technical expertise and tooling\\ntranslating requirements of reporting users for platform backend developers\\nQualifications\\nSound programming skills, preferably preferably Python + Scala\\nHigh proficiency in SQL\\nUnderstanding of ETL concepts, tools and technologies (e.g. Dataflow, Airflow)\\nUnderstanding of DWH concepts, architecture and technologies (e.g. Redshift, BigQuery)\\nPast experience with cloud technologies (preferably GCP)\\nExperience with Docker and Kubernetes\\nEnthusiasm and willingness to learn and keep up to speed with modern “big data” tech\\nAnalytical mind and drive to back decisions with data\\nProfessional level of English, additional languages is a plus\\n\\nNice to have :\\nUnderstanding of web and mobile tracking technologies (GA, GTM, Adjust, Leanplum)\\nUnderstanding of ML methods and tools (model creation, Scikit-learn)\\nExperience with event driven data processing (Kafka, Kinesis)\\nExperience with AWS stack (EC2, DMS, Athena...)\\nExperience with other BigData tools (e.g. Spark, Hadoop)\\nAdditional Information\\nWhat we offer:\\nAttractive perks – access to Quandoo’s pension scheme as well as various fitness and lifestyle benefits\\nModern equipment – state-of-the-art laptop & tools needed for optimal work results\\nFun working environment – opportunity to collaborate with highly motivated colleagues and attend company parties, the weekly Qweekend and Quandoo’s Global Summit\\nWork-home balance – flexible working hours, home office possibilities and up to 30 days of vacation\\nHigh level of responsibility – the chance to hit the ground running from day one and shape your own career path\\nWant to be part of Quandoo’s future? Apply now!\\nHeute\\n- Weiter zur Bewerbung\\nDiesen Job melden')     \n",
      "-------------END------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "tags = similar_documents\n",
    "tags = [list(i) for i in tags]\n",
    "\n",
    "print(f\"{tags}\\n\")\n",
    "# print(f\"{df_eng['job_title'][text_index], df_eng['company'][text_index], df_eng['job_text'][text_index]} \\\n",
    "#     \\n-------------END------------\\n \")\n",
    "\n",
    "for tag in tags: \n",
    "    num = int(tag[0].strip('tag_'))\n",
    "\n",
    "    print(f\"{df_eng['job_title'][num], df_eng['company'][num], df_eng['job_text'][num]} \\\n",
    "    \\n-------------END------------\\n \") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-loading",
   "metadata": {},
   "source": [
    "## improve the model 1\n",
    "Steps:\n",
    "- filter out all words not in dictionary \n",
    "- train model\n",
    "- get output and see if it's better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-gothic",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dictionary\n",
    "\n",
    "with open('../fydjob/data/dicts/skills_dict.json') as json_file:\n",
    "    dictionary = json.load(json_file)\n",
    "\n",
    "# collapse dictionary into list\n",
    "skills_list = [item for key, value in dictionary.items() for item in value]\n",
    "#print(sorted(skills_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-indonesian",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter tokens for skill\n",
    "filtered_texts = [[word for word in text if word in skills_list] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag documents\n",
    "filtered_texts_tag = tag_texts(filtered_texts)\n",
    "filtered_texts_tag_small = filtered_texts_tag[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-reset",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model # train_model.train(alldocs, total_examples=len(alldocs), epochs=epochs, start_alpha=0.025, end_alpha=0.001)\n",
    "\n",
    "data_to_train = filtered_texts_tag_small # texts_tagged_small, texts_tagged, filtered_texts_tag_small\n",
    "\n",
    "# build vocabulary with CBOW (dm=0)\n",
    "model_dbow = Doc2Vec(documents=data_to_train,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(data_to_train), \n",
    "                     min_count=1)\n",
    "\n",
    "# train the model\n",
    "model_dbow.train(data_to_train, total_examples=model_dbow.corpus_count, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-crystal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dbow.save('../../../models/doc2vec_filtered_5000_50_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "model_loaded = Doc2Vec.load('../../../models/doc2vec_filtered_5000_50_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-mineral",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = 770\n",
    "print(f\"{df_eng['job_title'][num], df_eng['company'][num], df_eng['job_text'][num]}\\n {filtered_texts[num]} -------------END------------\\n \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-documentation",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_top_jobs(filtered_texts[num], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-revision",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "offer = '''Description\n",
    "\n",
    "Would you like to join the team that protects the global AWS platform from fraud? Do you enjoy thinking like a fraudster and using your technical skills to help detect & mitigate AWS accounts from being compromised? If so, AWS Fraud Prevention has an exciting opportunity for you.\n",
    "\n",
    "AWS has the most services and more features within those services, than any other cloud provider–from infrastructure technologies like compute, storage, and databases–to emerging technologies, such as machine learning and artificial intelligence, data lakes and analytics, and Internet of Things. AWS Platform is the glue that holds the AWS ecosystem together. Whether its Identity features such as access management and sign on, cryptography, console, builder & developer tools, and even projects like automating all of our contractual billing systems, AWS Platform is always innovating with the customer in mind. The AWS Platform team sustains over 750 million transactions per second.\n",
    "\n",
    "The AWS Fraud Prevention Compromise vertical is responsible for detecting & mitigating AWS account compromise. You’ll be part of a team of Data Scientists, Investigations Analysts, and Technical & non-Technical Program Managers. The team’s goal is to identify and neutralize fraudsters from compromising AWS customers’ accounts.\n",
    "\n",
    "As a Data Scientist, you will work directly with Business Analysts and Software Development Engineers to monitor the flavor/ trend of compromise on AWS worldwide and design appropriate solutions to respond in a collaborative environment. There are no walls, and success is determined by your ability to dive deep, and understand the subtle demands new and complex services will place upon systems and teams.\n",
    "\n",
    "As a Data Scientist Your Responsibilities Will Include\n",
    "Apply state-of-the-art Machine Learning methods to large amounts of data from different sources to build and productionalize fraud prevention, detection and mitigation solutions\n",
    "Deep dive on the problems using SQL and scripting languages like Python/R to drive short term and long term solutions leveraging Statistical Analysis\n",
    "Analyze data (past customer behavior, sales inputs, and other sources) to figure out trends, create compromise prevention and mitigation solutions and output reports with clear recommendations\n",
    "Collaborate closely with the development team to recommend and build innovations based on Data Science\n",
    "Manage your own process: identify and execute on high impact projects, triage external requests, and make sure you bring projects to conclusion in time for the results to be useful\n",
    "Learn and Be Curious. We have a formal mentor search application that lets you find a mentor that works best for you based on location, job family, job level etc. Your manager can also help you find a mentor or two, because two is better than one. In addition to formal mentors, we work and train together so that we are always learning from one another, and we celebrate and support the career progression of our team members.\n",
    "\n",
    "Inclusion and Diversity. Our team is diverse! We drive towards an inclusive culture and work environment. We are intentional about attracting, developing, and retaining amazing talent from diverse backgrounds. Team members are active in Amazon’s 10+ affinity groups, sometimes known as employee resource groups, which bring employees together across businesses and locations around the world. These range from groups such as the Black Employee Network, Latinos at Amazon, Indigenous at Amazon, Families at Amazon, Amazon Women and Engineering, LGBTQ+, Warriors at Amazon (Military), Amazon People With Disabilities, and more.\n",
    "\n",
    "Learn more about Amazon on our Day 1 Blog: https://blog.aboutamazon.com\n",
    "\n",
    "\n",
    "Basic Qualifications\n",
    "Master’s degree in Mathematics, Statistics, Computer Science or in another related field\n",
    "Several years of hands-on relevant experience using programming/scripting languages such as Python or equivalent\n",
    "Proven understanding of Statistical Analysis, Modeling and Machine Learning techniques\n",
    "Experience in designing and deploying ML modeling and prediction pipelines\n",
    "Ability to leverage SQL or Spark for Ad-hoc analyses and building out ETL pipelines on heterogeneous data sources\n",
    "Experience performing statistical analysis and using tools such as R, pandas, or equivalent\n",
    "Preferred Qualifications\n",
    "Experience and proficiency with AWS technologies (EC2, CloudTrail, S3, SageMaker, Lambda, DynamoDB, RDS, etc.), and Big Data technologies\n",
    "Familiarity with AWS Redshift, Spark or other distributed computing technologies\n",
    "Previous work as a Data Scientist in the context of fraud analytics or risk scoring\n",
    "Ability to work in a fast-paced, ambiguous environment while prioritizing and managing multiple responsibilities\n",
    "Excellent written and verbal communication skills\n",
    "Excellent problem solving skills with a attention to detail\n",
    "Amazon is an equal opportunities employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. Protecting your privacy and the security of your data is a longstanding top priority for Amazon. Please consult our Privacy Notice to know more about how we collect, use and transfer the personal data of our candidates.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_offer = to_lower(offer)\n",
    "token_offer = remove_number(token_offer)\n",
    "token_offer = remove_punctuation(token_offer)\n",
    "token_offer = lemmatize_words(token_offer)\n",
    "token_offer = remove_stopwords(token_offer)\n",
    "token_offer = [word for word in token_offer if word in skills_list]\n",
    "print(token_offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offer needs to b masked as well!!\n",
    "\n",
    "print_top_jobs(token_offer, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-leonard",
   "metadata": {},
   "source": [
    "# Dummy copy-paste model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fydjob.Doc2VecPipeline import Doc2VecPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2VecPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-acting",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "offer = '''Description\n",
    "\n",
    "Would you like to join the team that protects the global AWS platform from fraud? Do you enjoy thinking like a fraudster and using your technical skills to help detect & mitigate AWS accounts from being compromised? If so, AWS Fraud Prevention has an exciting opportunity for you.\n",
    "\n",
    "AWS has the most services and more features within those services, than any other cloud provider–from infrastructure technologies like compute, storage, and databases–to emerging technologies, such as machine learning and artificial intelligence, data lakes and analytics, and Internet of Things. AWS Platform is the glue that holds the AWS ecosystem together. Whether its Identity features such as access management and sign on, cryptography, console, builder & developer tools, and even projects like automating all of our contractual billing systems, AWS Platform is always innovating with the customer in mind. The AWS Platform team sustains over 750 million transactions per second.\n",
    "\n",
    "The AWS Fraud Prevention Compromise vertical is responsible for detecting & mitigating AWS account compromise. You’ll be part of a team of Data Scientists, Investigations Analysts, and Technical & non-Technical Program Managers. The team’s goal is to identify and neutralize fraudsters from compromising AWS customers’ accounts.\n",
    "\n",
    "As a Data Scientist, you will work directly with Business Analysts and Software Development Engineers to monitor the flavor/ trend of compromise on AWS worldwide and design appropriate solutions to respond in a collaborative environment. There are no walls, and success is determined by your ability to dive deep, and understand the subtle demands new and complex services will place upon systems and teams.\n",
    "\n",
    "As a Data Scientist Your Responsibilities Will Include\n",
    "Apply state-of-the-art Machine Learning methods to large amounts of data from different sources to build and productionalize fraud prevention, detection and mitigation solutions\n",
    "Deep dive on the problems using SQL and scripting languages like Python/R to drive short term and long term solutions leveraging Statistical Analysis\n",
    "Analyze data (past customer behavior, sales inputs, and other sources) to figure out trends, create compromise prevention and mitigation solutions and output reports with clear recommendations\n",
    "Collaborate closely with the development team to recommend and build innovations based on Data Science\n",
    "Manage your own process: identify and execute on high impact projects, triage external requests, and make sure you bring projects to conclusion in time for the results to be useful\n",
    "Learn and Be Curious. We have a formal mentor search application that lets you find a mentor that works best for you based on location, job family, job level etc. Your manager can also help you find a mentor or two, because two is better than one. In addition to formal mentors, we work and train together so that we are always learning from one another, and we celebrate and support the career progression of our team members.\n",
    "\n",
    "Inclusion and Diversity. Our team is diverse! We drive towards an inclusive culture and work environment. We are intentional about attracting, developing, and retaining amazing talent from diverse backgrounds. Team members are active in Amazon’s 10+ affinity groups, sometimes known as employee resource groups, which bring employees together across businesses and locations around the world. These range from groups such as the Black Employee Network, Latinos at Amazon, Indigenous at Amazon, Families at Amazon, Amazon Women and Engineering, LGBTQ+, Warriors at Amazon (Military), Amazon People With Disabilities, and more.\n",
    "\n",
    "Learn more about Amazon on our Day 1 Blog: https://blog.aboutamazon.com\n",
    "\n",
    "\n",
    "Basic Qualifications\n",
    "Master’s degree in Mathematics, Statistics, Computer Science or in another related field\n",
    "Several years of hands-on relevant experience using programming/scripting languages such as Python or equivalent\n",
    "Proven understanding of Statistical Analysis, Modeling and Machine Learning techniques\n",
    "Experience in designing and deploying ML modeling and prediction pipelines\n",
    "Ability to leverage SQL or Spark for Ad-hoc analyses and building out ETL pipelines on heterogeneous data sources\n",
    "Experience performing statistical analysis and using tools such as R, pandas, or equivalent\n",
    "Preferred Qualifications\n",
    "Experience and proficiency with AWS technologies (EC2, CloudTrail, S3, SageMaker, Lambda, DynamoDB, RDS, etc.), and Big Data technologies\n",
    "Familiarity with AWS Redshift, Spark or other distributed computing technologies\n",
    "Previous work as a Data Scientist in the context of fraud analytics or risk scoring\n",
    "Ability to work in a fast-paced, ambiguous environment while prioritizing and managing multiple responsibilities\n",
    "Excellent written and verbal communication skills\n",
    "Excellent problem solving skills with a attention to detail\n",
    "Amazon is an equal opportunities employer. We believe passionately that employing a diverse workforce is central to our success. We make recruiting decisions based on your experience and skills. We value your passion to discover, invent, simplify and build. Protecting your privacy and the security of your data is a longstanding top priority for Amazon. Please consult our Privacy Notice to know more about how we collect, use and transfer the personal data of our candidates.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.find_similar_jobs_from_string(offer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-anxiety",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

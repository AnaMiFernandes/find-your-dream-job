{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-aberdeen",
   "metadata": {},
   "source": [
    "# Packaging tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #\n",
    "import json #\n",
    "import multiprocessing#\n",
    "\n",
    "import joblib \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-trunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rocky-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('../../../raw_data/processed_data.joblib')\n",
    "df['tag_language'] = df['tag_language'].fillna(value='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qualified-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select english jobs\n",
    "df_eng = df.copy()\n",
    "df_eng = df_eng[df_eng['tag_language'] == 'en']\n",
    "df_eng.reset_index(inplace=True)\n",
    "df_eng.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ancient-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings\n",
    "def join_strings(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_words(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thermal-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "df_eng['clean'] = df_eng['job_text_tokenized'].apply(join_strings).apply(lemmatize_words)\\\n",
    "    .apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-papua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extended-license",
   "metadata": {},
   "source": [
    "# functions for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "multiple-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_texts(texts):\n",
    "    ''' input: pandas series of datasets to use in the model\n",
    "        returns texts with identification tag\n",
    "    '''\n",
    "    texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "    return texts_tagged\n",
    "\n",
    "texts_tagged = tag_texts(texts)\n",
    "#texts_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "laden-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tagged_small = texts_tagged[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "criminal-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate_Doc2Vec(text_tagged):\n",
    "    '''instanciates model, using dbow (d=0)'''\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model_dbow = Doc2Vec(documents=text_tagged,\n",
    "                         dm=0,\n",
    "                         alpha=0.025,\n",
    "                         vector_size=len(df), \n",
    "                         min_count=1,\n",
    "                         workers=cores,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "incomplete-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Doc2Vec(texts_tagged_small):\n",
    "    ''' trains model'''\n",
    "    model_dbow.train(text_tagged,\n",
    "                     total_examples=model_dbow.corpus_count, \n",
    "                     epochs=15,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Doc2Vec(path):\n",
    "    '''saves trained Doc2Vec\n",
    "    input: path to location \n",
    "    '''\n",
    "    model_dbow.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_train = texts_tagged_small # texts_tagged_small, texts_tagged\n",
    "\n",
    "# build vocabulary with CBOW (dm=0)\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_dbow = Doc2Vec(documents=data_to_train,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(data_to_train), \n",
    "                     min_count=1,\n",
    "                     workers=cores\n",
    "                    )\n",
    "\n",
    "# train the model\n",
    "model_dbow.train(data_to_train, total_examples=model_dbow.corpus_count, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "enormous-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer vector and get similarity\n",
    "def get_similar_jobs(tokenized_job, number_offers):\n",
    "    ''' input: tokenized job offers, number of offers \n",
    "        returns tags of top x most similar job offers and similarity probabilities\n",
    "    '''\n",
    "\n",
    "    # infer vector from text \n",
    "    infer_vector = model_loaded.infer_vector(tokenized_job)\n",
    "    # find similar offers\n",
    "    similar_documents = model_loaded.docvecs.most_similar([infer_vector], topn = number_offers)\n",
    "\n",
    "    return similar_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-harvest",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-charles",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "obvious-newspaper",
   "metadata": {},
   "source": [
    "## start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "handmade-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec_Pipeline:\n",
    "    \n",
    "    # instanciate class\n",
    "    def __init__(self, path=None, texts=None):\n",
    "        if path:\n",
    "            self.d2v_model = Doc2Vec.load(path)\n",
    "        else:\n",
    "            cores = multiprocessing.cpu_count()\n",
    "            self.d2v_model = Doc2Vec(documents=df,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(df), \n",
    "                     min_count=1,\n",
    "                     workers=cores)\n",
    "    \n",
    "    # build vocabulary\n",
    "    def tag_docs(self, texts):\n",
    "        ''' input: pandas series of datasets to use in the model\n",
    "            returns texts with identification tag\n",
    "        '''\n",
    "        self.texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "        return self.texts_tagged\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    # define most similar jobs\n",
    "    \n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-hierarchy",
   "metadata": {},
   "source": [
    "# Packaging tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reverse-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #\n",
    "import json #\n",
    "import multiprocessing#\n",
    "\n",
    "import joblib \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tested-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('../../../raw_data/processed_data.joblib')\n",
    "df['tag_language'] = df['tag_language'].fillna(value='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select english jobs\n",
    "df_eng = df.copy()\n",
    "df_eng = df_eng[df_eng['tag_language'] == 'en']\n",
    "df_eng.reset_index(inplace=True)\n",
    "df_eng.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ignored-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings\n",
    "def join_strings(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_words(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solar-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "df_eng['clean'] = df_eng['job_text_tokenized'].apply(join_strings).apply(lemmatize_words)\\\n",
    "    .apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-cause",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ethical-insider",
   "metadata": {},
   "source": [
    "# functions for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-cookbook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "durable-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_texts(texts):\n",
    "    ''' input: pandas series of datasets to use in the model\n",
    "        returns texts with identification tag\n",
    "    '''\n",
    "    texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "    return texts_tagged\n",
    "\n",
    "texts_tagged = tag_texts(texts)\n",
    "#texts_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "negative-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tagged_small = texts_tagged[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "executive-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate_Doc2Vec(text_tagged):\n",
    "    '''instanciates model, using dbow (d=0)'''\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model_dbow = Doc2Vec(documents=text_tagged,\n",
    "                         dm=0,\n",
    "                         alpha=0.025,\n",
    "                         vector_size=len(df), \n",
    "                         min_count=1,\n",
    "                         workers=cores,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "available-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Doc2Vec(texts_tagged_small):\n",
    "    ''' trains model'''\n",
    "    model_dbow.train(text_tagged,\n",
    "                     total_examples=model_dbow.corpus_count, \n",
    "                     epochs=15,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Doc2Vec(path):\n",
    "    '''saves trained Doc2Vec\n",
    "    input: path to location \n",
    "    '''\n",
    "    model_dbow.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sound-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer vector and get similarity\n",
    "def get_similar_jobs(tokenized_job, number_offers=10):\n",
    "    ''' input: tokenized job offers, number of offers \n",
    "        returns tags of top x most similar job offers and similarity probabilities\n",
    "    '''\n",
    "    # infer vector from text \n",
    "    infer_vector = model_loaded.infer_vector(tokenized_job)\n",
    "    # find similar offers\n",
    "    similar_documents = model_loaded.docvecs.most_similar([infer_vector], topn = number_offers)\n",
    "\n",
    "    return similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-lease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-genome",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-sound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "figured-petite",
   "metadata": {},
   "source": [
    "## start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecological-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec_Pipeline:\n",
    "    \n",
    "    # instanciate class\n",
    "    def __init__(self, path=None, texts=None):\n",
    "        if path:\n",
    "            self.d2v_model = Doc2Vec.load(path)\n",
    "        else:\n",
    "            cores = multiprocessing.cpu_count()\n",
    "            self.d2v_model = Doc2Vec(documents=df,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(df), \n",
    "                     min_count=1,\n",
    "                     workers=cores)\n",
    "    \n",
    "    # build vocabulary\n",
    "    def tag_docs(self, texts):\n",
    "        ''' input: pandas series of datasets to use in the model\n",
    "            returns texts with identification tag\n",
    "        '''\n",
    "        self.texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "        return self.texts_tagged\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    # define most similar jobs\n",
    "    \n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

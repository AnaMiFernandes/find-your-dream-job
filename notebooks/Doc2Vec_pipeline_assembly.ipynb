{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baking-alexander",
   "metadata": {},
   "source": [
    "# Packaging tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flying-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument #\n",
    "import json #\n",
    "import multiprocessing#\n",
    "\n",
    "# import joblib \n",
    "# import pandas as pd\n",
    "# from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import joblib\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# from pprint import pprint\n",
    "# import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optimum-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('../../../raw_data/processed_data.joblib')\n",
    "df['tag_language'] = df['tag_language'].fillna(value='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absent-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select english jobs\n",
    "df_eng = df.copy()\n",
    "df_eng = df_eng[df_eng['tag_language'] == 'en']\n",
    "df_eng.reset_index(inplace=True)\n",
    "df_eng.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driven-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join strings\n",
    "def join_strings(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize_words(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supposed-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "df_eng['clean'] = df_eng['job_text_tokenized'].apply(join_strings).apply(lemmatize_words)\\\n",
    "    .apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-fabric",
   "metadata": {},
   "source": [
    "# functions for pipeline (ignore code above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-volume",
   "metadata": {},
   "source": [
    "**Workflow:**\n",
    "\n",
    "1. load the model if present. if not, instanciate model\n",
    "2. add a tag to all job offers\n",
    "3. train model on tagged texts (in the saved model I trained it on 3000 datapoints, 15 epochs)\n",
    "4. save the model (if model not present)\n",
    "5. find similar job offers:\n",
    "    get recomendation based on copy-pasted offer or get recomendation based on ???? (decide this); \n",
    "    output of top 10 (or another number) similar to input offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dutch-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Doc2Vec(path_to_files):\n",
    "    '''loads and returns pre-trained model'''\n",
    "    model_dbow = Doc2Vec.load(path_to_files)\n",
    "    \n",
    "    return model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fancy-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanciate_Doc2Vec(text_tagged):\n",
    "    '''instanciates model, using dbow (d=0)'''\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model_dbow = Doc2Vec(documents=text_tagged,\n",
    "                         dm=0,\n",
    "                         alpha=0.025,\n",
    "                         vector_size=len(text_tagged), \n",
    "                         min_count=1,\n",
    "                         workers=cores,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mysterious-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_texts(texts):\n",
    "    ''' input: pandas series of datasets to use in the model\n",
    "        returns texts with identification tag\n",
    "    '''\n",
    "    texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "    return texts_tagged\n",
    "\n",
    "texts_tagged = tag_texts(texts)\n",
    "#texts_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "municipal-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Doc2Vec(texts_tagged_small):\n",
    "    ''' trains model'''\n",
    "    model_dbow.train(text_tagged,\n",
    "                     total_examples=model_dbow.corpus_count, \n",
    "                     epochs=15,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Doc2Vec(path):\n",
    "    '''saves trained Doc2Vec\n",
    "    input: path to location \n",
    "    '''\n",
    "    model_dbow.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bright-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer vector and get similarity\n",
    "def find_similar_jobs(model, tokenized_offers, number_offers=10):\n",
    "    ''' inputs: Doc2Vec model, string of a tokenized job offer, number of offers. \n",
    "        returns: tags of top x most similar job offers and similarity probabilities\n",
    "    '''\n",
    "    # infer vector from text \n",
    "    infer_vector = model.infer_vector(tokenized_offers)\n",
    "    # find similar offers\n",
    "    similar_documents = model.docvecs.most_similar([infer_vector], topn = number_offers)\n",
    "\n",
    "    return similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "computational-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tagged_small = texts_tagged[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "headed-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_Doc2Vec('../../../models/doc2vec_3000_15_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "controversial-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tag_272', 0.9891966581344604),\n",
       " ('tag_754', 0.9883853197097778),\n",
       " ('tag_0', 0.9864751100540161),\n",
       " ('tag_654', 0.7715011835098267),\n",
       " ('tag_368', 0.7501724362373352),\n",
       " ('tag_382', 0.7268041372299194),\n",
       " ('tag_100', 0.6558924913406372),\n",
       " ('tag_723', 0.6515829563140869),\n",
       " ('tag_65', 0.6219576597213745),\n",
       " ('tag_111', 0.6144018173217773)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_jobs(model, texts_tagged_small[0][0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-detection",
   "metadata": {},
   "source": [
    "## start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "powered-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec_Pipeline:\n",
    "    \n",
    "    # instanciate class\n",
    "    def __init__(self, path=None, texts=None):\n",
    "        if path:\n",
    "            self.d2v_model = Doc2Vec.load(path)\n",
    "        else:\n",
    "            cores = multiprocessing.cpu_count()\n",
    "            self.d2v_model = Doc2Vec(documents=df,\n",
    "                     dm=0,\n",
    "                     alpha=0.025,\n",
    "                     vector_size=len(df), \n",
    "                     min_count=1,\n",
    "                     workers=cores)\n",
    "    \n",
    "    # build vocabulary\n",
    "    def tag_docs(self, texts):\n",
    "        ''' input: pandas series of datasets to use in the model\n",
    "            returns texts with identification tag\n",
    "        '''\n",
    "        self.texts_tagged = [TaggedDocument(text, tags=['tag_'+str(tag)]) for tag, text in enumerate(texts)]\n",
    "\n",
    "        return self.texts_tagged\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    # define most similar jobs\n",
    "    \n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

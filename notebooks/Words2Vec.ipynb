{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metropolitan-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-cbc447fde3a3>\", line 9, in <module>\n",
      "    df = joblib.load(\"../fydjob/output/indeed_proc/ip_2021-03-05.joblib\")\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 585, in load\n",
      "    obj = _unpickle(fobj, filename, mmap_mode)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/alexanderherdt/.pyenv/versions/3.8.6/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cbc447fde3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../fydjob/output/indeed_proc/ip_2021-03-05.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, TFPreTrainedModel  \n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "df = joblib.load(\"../fydjob/output/indeed_proc/ip_2021-03-05.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "accompanied-sussex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_text</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_info</th>\n",
       "      <th>query_text</th>\n",
       "      <th>source</th>\n",
       "      <th>job_link</th>\n",
       "      <th>tag_language</th>\n",
       "      <th>reviews</th>\n",
       "      <th>job_info_tokenized</th>\n",
       "      <th>job_text_tokenized</th>\n",
       "      <th>job_text_tokenized_titlecase</th>\n",
       "      <th>job_title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Junior) Data Engineer (f/m/x)</td>\n",
       "      <td>Customlytics ist die führende App Marketing Be...</td>\n",
       "      <td>Customlytics GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>(Junior) Data Engineer (f/m/x)\\nCustomlytics G...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[junior, data, engineer, fmx, customlytics, gm...</td>\n",
       "      <td>[customlytics, ist, die, führende, app, market...</td>\n",
       "      <td>[Customlytics, ist, die, führende, App, Market...</td>\n",
       "      <td>[junior, data, engineer, fmx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Responsibilities\\n\\nAs working student (m/f/x)...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin]</td>\n",
       "      <td>[responsibilities, as, working, student, mfx, ...</td>\n",
       "      <td>[Responsibilities, As, working, student, mfx, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin, teilzeit, pr...</td>\n",
       "      <td>[aufgaben, als, werkstudent, mwd, it, arbeites...</td>\n",
       "      <td>[Aufgaben, Als, Werkstudent, mwd, IT, arbeites...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Startdatum\\n\\nAb sofort\\n\\nDeine Aufgaben\\n\\nD...</td>\n",
       "      <td>Alexander Thamm GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Alexander Thamm GmbH - Berlin\\nFestanstellung</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alexander, thamm, gmbh, berlin, festanstellung]</td>\n",
       "      <td>[startdatum, ab, sofort, deine, aufgaben, dein...</td>\n",
       "      <td>[Startdatum, Ab, sofort, Deine, Aufgaben, Dein...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Developer (m/f/d)</td>\n",
       "      <td>We’re Phiture: a leading mobile growth consult...</td>\n",
       "      <td>Phiture</td>\n",
       "      <td>BerlinKreuzberg</td>\n",
       "      <td>Full Stack Developer (m/f/d)\\nPhiture - Berlin...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[full, stack, developer, mfd, phiture, berlink...</td>\n",
       "      <td>[were, phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[Were, Phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[full, stack, developer, mfd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        job_title  \\\n",
       "0  (Junior) Data Engineer (f/m/x)   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4    Full Stack Developer (m/f/d)   \n",
       "\n",
       "                                            job_text               company  \\\n",
       "0  Customlytics ist die führende App Marketing Be...     Customlytics GmbH   \n",
       "1  Responsibilities\\n\\nAs working student (m/f/x)...            Aroundhome   \n",
       "2  Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...            Aroundhome   \n",
       "3  Startdatum\\n\\nAb sofort\\n\\nDeine Aufgaben\\n\\nD...  Alexander Thamm GmbH   \n",
       "4  We’re Phiture: a leading mobile growth consult...               Phiture   \n",
       "\n",
       "          location                                           job_info  \\\n",
       "0           Berlin  (Junior) Data Engineer (f/m/x)\\nCustomlytics G...   \n",
       "1           Berlin                   Aroundhome6 Bewertungen - Berlin   \n",
       "2           Berlin  Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...   \n",
       "3           Berlin      Alexander Thamm GmbH - Berlin\\nFestanstellung   \n",
       "4  BerlinKreuzberg  Full Stack Developer (m/f/d)\\nPhiture - Berlin...   \n",
       "\n",
       "     query_text       source job_link tag_language  reviews  \\\n",
       "0  data science  scrape_json      NaN           en      NaN   \n",
       "1  data science  scrape_json      NaN           en      NaN   \n",
       "2  data science  scrape_json      NaN           de      NaN   \n",
       "3  data science  scrape_json      NaN           de      NaN   \n",
       "4  data science  scrape_json      NaN           en      NaN   \n",
       "\n",
       "                                  job_info_tokenized  \\\n",
       "0  [junior, data, engineer, fmx, customlytics, gm...   \n",
       "1                  [aroundhome, bewertungen, berlin]   \n",
       "2  [aroundhome, bewertungen, berlin, teilzeit, pr...   \n",
       "3   [alexander, thamm, gmbh, berlin, festanstellung]   \n",
       "4  [full, stack, developer, mfd, phiture, berlink...   \n",
       "\n",
       "                                  job_text_tokenized  \\\n",
       "0  [customlytics, ist, die, führende, app, market...   \n",
       "1  [responsibilities, as, working, student, mfx, ...   \n",
       "2  [aufgaben, als, werkstudent, mwd, it, arbeites...   \n",
       "3  [startdatum, ab, sofort, deine, aufgaben, dein...   \n",
       "4  [were, phiture, a, leading, mobile, growth, co...   \n",
       "\n",
       "                        job_text_tokenized_titlecase  \\\n",
       "0  [Customlytics, ist, die, führende, App, Market...   \n",
       "1  [Responsibilities, As, working, student, mfx, ...   \n",
       "2  [Aufgaben, Als, Werkstudent, mwd, IT, arbeites...   \n",
       "3  [Startdatum, Ab, sofort, Deine, Aufgaben, Dein...   \n",
       "4  [Were, Phiture, a, leading, mobile, growth, co...   \n",
       "\n",
       "             job_title_tokenized  \n",
       "0  [junior, data, engineer, fmx]  \n",
       "1                             []  \n",
       "2                             []  \n",
       "3                             []  \n",
       "4  [full, stack, developer, mfd]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-calgary",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Baseline Words2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "frozen-andorra",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 0.9542497396469116),\n",
       " ('java', 0.9488599300384521),\n",
       " ('scala', 0.9480288028717041),\n",
       " ('matlab', 0.9360655546188354),\n",
       " ('perl', 0.9264839887619019),\n",
       " ('julia', 0.8986462354660034),\n",
       " ('programing', 0.896690309047699),\n",
       " ('cc', 0.8916913866996765),\n",
       " ('programming', 0.8786020278930664),\n",
       " ('clojure', 0.8782665729522705)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### instanciate model\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(df[\"job_text_tokenized\"], progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(df[\"job_text_tokenized\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "brown-attribute",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'I want a job that involves' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-f6ccf9a76ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I want a job that involves\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'I want a job that involves' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v_model.wv.most_similar([\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-affiliation",
   "metadata": {},
   "source": [
    "## Training a Words2vec model with bi-gram parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-matrix",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "'''\n",
    "Preprocessing for the Job descriptions in paresed in senteneces.\n",
    "Modified form the other preprocessing pipeline\n",
    "'''\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation_mod(text):\n",
    "    \n",
    "    punct = string.punctuation.replace(\".\",\"\")\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    for sent in text:\n",
    "        sent = word_tokenize(sent) \n",
    "        sent = [w for w in sent if w not in stop_words and w not in string.punctuation and w]  \n",
    "        new_text.append(sent)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out german offers\n",
    "df = df[df[\"tag_language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply preprocessing to df\n",
    "df[\"job_text_sent\"]= df[\"job_text\"].apply(to_lower).apply(remove_number)\\\n",
    "                                    .apply(lambda x : x.replace('\\n',' '))\\\n",
    "                                    .apply(remove_punctuation_mod)\\\n",
    "                                    .apply(lambda x: sent_tokenize(x))\\\n",
    "                                    .apply(remove_stopwords)\\\n",
    "                                    .apply(lemmatize_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cosmetic-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['customlytics',\n",
       "  'ist',\n",
       "  'die',\n",
       "  'führende',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'beratungsagentur',\n",
       "  'aus',\n",
       "  'berlin'],\n",
       " ['wir',\n",
       "  'bieten',\n",
       "  'consulting',\n",
       "  'und',\n",
       "  'handson',\n",
       "  'support',\n",
       "  'rund',\n",
       "  'um',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'strategie',\n",
       "  'produktmanagement',\n",
       "  'analytics',\n",
       "  'crm'],\n",
       " ['unser',\n",
       "  'team',\n",
       "  'erarbeitet',\n",
       "  'mit',\n",
       "  'unternehmen',\n",
       "  'jeder',\n",
       "  'größe',\n",
       "  'konzepte',\n",
       "  'zur',\n",
       "  'erfolgreichen',\n",
       "  'vermarktung',\n",
       "  'von',\n",
       "  'mobilen',\n",
       "  'apps'],\n",
       " ['dabei',\n",
       "  'decken',\n",
       "  'wir',\n",
       "  'nicht',\n",
       "  'nur',\n",
       "  'das',\n",
       "  'gesamte',\n",
       "  'spektrum',\n",
       "  'infrastruktureller',\n",
       "  'marketingthemen',\n",
       "  'ab'],\n",
       " ['wir',\n",
       "  'konzipieren',\n",
       "  'planen',\n",
       "  'und',\n",
       "  'steuern',\n",
       "  'sowohl',\n",
       "  'das',\n",
       "  'ui',\n",
       "  'ux',\n",
       "  'design',\n",
       "  'von',\n",
       "  'mobilen',\n",
       "  'apps',\n",
       "  'als',\n",
       "  'auch',\n",
       "  'performance',\n",
       "  'marketing',\n",
       "  'kampagnen',\n",
       "  'für',\n",
       "  'alle',\n",
       "  'app',\n",
       "  'verticals'],\n",
       " ['über', 'uns', 'unser', 'data', 'team', 'braucht', 'unterstützung'],\n",
       " ['du',\n",
       "  'bist',\n",
       "  'motiviert',\n",
       "  'und',\n",
       "  'von',\n",
       "  'der',\n",
       "  'mobile',\n",
       "  'industry',\n",
       "  'begeistert',\n",
       "  'dann',\n",
       "  'suchen',\n",
       "  'wir',\n",
       "  'dich',\n",
       "  'um',\n",
       "  'die',\n",
       "  'data',\n",
       "  'warehouselösungen',\n",
       "  'für',\n",
       "  'unsere',\n",
       "  'kunden',\n",
       "  'aus',\n",
       "  'der',\n",
       "  'app',\n",
       "  'industrie',\n",
       "  'zu',\n",
       "  'entwickeln',\n",
       "  'und',\n",
       "  'implementieren'],\n",
       " ['zusammen',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'biteam',\n",
       "  'arbeitest',\n",
       "  'du',\n",
       "  'kundenprojekten',\n",
       "  'der',\n",
       "  'entwicklung',\n",
       "  'implementierung',\n",
       "  'und',\n",
       "  'optimierung',\n",
       "  'von',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'projekten'],\n",
       " ['du',\n",
       "  'arbeitest',\n",
       "  'hierbei',\n",
       "  'eng',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'big',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'und',\n",
       "  'data',\n",
       "  'analyst',\n",
       "  'zusammen',\n",
       "  'der',\n",
       "  'entwicklung',\n",
       "  'von',\n",
       "  'dashboards',\n",
       "  'und',\n",
       "  'data',\n",
       "  'visualisierungen'],\n",
       " ['talent',\n",
       "  'technologieaffinität',\n",
       "  'und',\n",
       "  'eigenverantwortung',\n",
       "  'für',\n",
       "  'deine',\n",
       "  'arbeit',\n",
       "  'zu',\n",
       "  'übernehmen',\n",
       "  'sind',\n",
       "  'die',\n",
       "  'basis',\n",
       "  'unserer',\n",
       "  'teamkultur'],\n",
       " ['für',\n",
       "  'uns',\n",
       "  'ist',\n",
       "  'es',\n",
       "  'zweitrangig',\n",
       "  'ob',\n",
       "  'du',\n",
       "  'erst',\n",
       "  'anfang',\n",
       "  'deiner',\n",
       "  'beruflichen',\n",
       "  'laufbahn',\n",
       "  'stehst',\n",
       "  'oder',\n",
       "  'bereits',\n",
       "  'mehrjährige',\n",
       "  'berufserfahrung',\n",
       "  'hast',\n",
       "  'der',\n",
       "  'spirit',\n",
       "  'und',\n",
       "  'die',\n",
       "  'motivation',\n",
       "  'zählen'],\n",
       " ['deine',\n",
       "  'aufgaben',\n",
       "  'du',\n",
       "  'berätst',\n",
       "  'unsere',\n",
       "  'teams',\n",
       "  'hinsichtlich',\n",
       "  'der',\n",
       "  'datenanforderungen',\n",
       "  'und',\n",
       "  'bist',\n",
       "  'die',\n",
       "  'schnittstelle',\n",
       "  'zwischen',\n",
       "  'marketingprodukt',\n",
       "  'consultants',\n",
       "  'und',\n",
       "  'dem',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'bi',\n",
       "  'team'],\n",
       " ['du',\n",
       "  'unterstützt',\n",
       "  'bei',\n",
       "  'der',\n",
       "  'analyse',\n",
       "  'von',\n",
       "  'datenströmen',\n",
       "  'unserer',\n",
       "  'kunden',\n",
       "  'aus',\n",
       "  'der',\n",
       "  'mobilen',\n",
       "  'app',\n",
       "  'branche',\n",
       "  'dem',\n",
       "  'aufbereiten',\n",
       "  'von',\n",
       "  'daten',\n",
       "  'und',\n",
       "  'dem',\n",
       "  'entwickeln',\n",
       "  'von',\n",
       "  'datenmodellen'],\n",
       " ['du',\n",
       "  'unterstützt',\n",
       "  'unser',\n",
       "  'consultingteam',\n",
       "  'und',\n",
       "  'interne',\n",
       "  'abteilungen',\n",
       "  'als',\n",
       "  'experte',\n",
       "  'bei',\n",
       "  'der',\n",
       "  'vorverarbeitung',\n",
       "  'und',\n",
       "  'auswertung',\n",
       "  'großer',\n",
       "  'datenmengen'],\n",
       " ['du',\n",
       "  'versetzt',\n",
       "  'dich',\n",
       "  'die',\n",
       "  'lage',\n",
       "  'deiner',\n",
       "  'externen',\n",
       "  'und',\n",
       "  'internen',\n",
       "  'stakeholder',\n",
       "  'verstehst',\n",
       "  'ihre',\n",
       "  'ziele',\n",
       "  'und',\n",
       "  'anforderungen',\n",
       "  'und',\n",
       "  'übersetzt',\n",
       "  'diese',\n",
       "  'lösungen'],\n",
       " ['entwicklung',\n",
       "  'von',\n",
       "  'data',\n",
       "  'warehousingprodukten',\n",
       "  'die',\n",
       "  'unseren',\n",
       "  'wettbewerbsvorteil',\n",
       "  'langfristig',\n",
       "  'sichern',\n",
       "  'und',\n",
       "  'ausbauen'],\n",
       " ['dein',\n",
       "  'profil',\n",
       "  'du',\n",
       "  'arbeitet',\n",
       "  'gerne',\n",
       "  'auf',\n",
       "  'englisch',\n",
       "  'mit',\n",
       "  'internationalen',\n",
       "  'kolleginnen',\n",
       "  'du',\n",
       "  'hast',\n",
       "  'erste',\n",
       "  'erfahrung',\n",
       "  'im',\n",
       "  'data',\n",
       "  'oder',\n",
       "  'bo',\n",
       "  'bereich',\n",
       "  'und',\n",
       "  'hast',\n",
       "  'ein',\n",
       "  'abgeschlossenes',\n",
       "  'studium',\n",
       "  'im',\n",
       "  'bereich',\n",
       "  'information',\n",
       "  'systems',\n",
       "  'analytics',\n",
       "  'wirtschaftsinformatik',\n",
       "  'mathematik',\n",
       "  'oder',\n",
       "  'statistik',\n",
       "  'mit',\n",
       "  'affinität',\n",
       "  'zu',\n",
       "  'betriebswirtschaftlichen',\n",
       "  'themen',\n",
       "  'wie',\n",
       "  'z.b'],\n",
       " ['online', 'marketing'],\n",
       " ['du',\n",
       "  'begeisterst',\n",
       "  'dich',\n",
       "  'für',\n",
       "  'die',\n",
       "  'mobile',\n",
       "  'apptechnologiebranche',\n",
       "  'und',\n",
       "  'hast',\n",
       "  'großes',\n",
       "  'interesse',\n",
       "  'technischen',\n",
       "  'lösungen',\n",
       "  'und',\n",
       "  'wie',\n",
       "  'apps',\n",
       "  'unter',\n",
       "  'der',\n",
       "  'haube',\n",
       "  'funktionieren'],\n",
       " ['basiswissen', 'sql', 'python', 'und', 'git'],\n",
       " ['grundkenntnisse',\n",
       "  'redshift',\n",
       "  'undoder',\n",
       "  'bigquery',\n",
       "  'routinierter',\n",
       "  'umgang',\n",
       "  'mit',\n",
       "  'gängigen',\n",
       "  'anwendungen',\n",
       "  'wie',\n",
       "  'google',\n",
       "  'drive',\n",
       "  'microsoft',\n",
       "  'office',\n",
       "  'mac',\n",
       "  'os',\n",
       "  'und',\n",
       "  'keine',\n",
       "  'scheu',\n",
       "  'sich',\n",
       "  'schnell',\n",
       "  'verschiedene',\n",
       "  'projektmanagement',\n",
       "  'analyse',\n",
       "  'und',\n",
       "  'marketingtools',\n",
       "  'einzuarbeiten'],\n",
       " ['du',\n",
       "  'überzeugst',\n",
       "  'mit',\n",
       "  'proaktivität',\n",
       "  'und',\n",
       "  'lösungsorientierter',\n",
       "  'denkweise'],\n",
       " ['bonus',\n",
       "  'points',\n",
       "  'agentur',\n",
       "  'undoder',\n",
       "  'startup',\n",
       "  'erfahrung',\n",
       "  'erste',\n",
       "  'erfahrungen',\n",
       "  'mit',\n",
       "  'reporting',\n",
       "  'tools',\n",
       "  'wie',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'und',\n",
       "  'der',\n",
       "  'google',\n",
       "  'cloud',\n",
       "  'platform',\n",
       "  'aws',\n",
       "  'wir',\n",
       "  'bieten',\n",
       "  'mehr',\n",
       "  'als',\n",
       "  'app',\n",
       "  'store',\n",
       "  'optimization',\n",
       "  'wir',\n",
       "  'sind',\n",
       "  'die',\n",
       "  'einzige',\n",
       "  'fullstack',\n",
       "  'beratungsagentur',\n",
       "  'im',\n",
       "  'deutschen',\n",
       "  'raum',\n",
       "  'und',\n",
       "  'beraten',\n",
       "  'kunden',\n",
       "  'zu',\n",
       "  'vielfältigen',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'produktmanagement',\n",
       "  'und',\n",
       "  'analytics',\n",
       "  'themen'],\n",
       " ['bvg', 'jobticket', 'für', 'den', 'ab', 'bereich'],\n",
       " ['highend',\n",
       "  'hardware',\n",
       "  'die',\n",
       "  'du',\n",
       "  'für',\n",
       "  'deine',\n",
       "  'perfekte',\n",
       "  'arbeitsumgebung',\n",
       "  'im',\n",
       "  'büro',\n",
       "  'und',\n",
       "  'remote',\n",
       "  'benötigst'],\n",
       " ['frei',\n",
       "  'verfügbare',\n",
       "  'anzahl',\n",
       "  'von',\n",
       "  'remote',\n",
       "  'work',\n",
       "  'tagen',\n",
       "  'ob',\n",
       "  'du',\n",
       "  'im',\n",
       "  'büro',\n",
       "  'arbeitest',\n",
       "  'oder',\n",
       "  'woanders',\n",
       "  'produktiver',\n",
       "  'bist',\n",
       "  'es',\n",
       "  'dir',\n",
       "  'selbst',\n",
       "  'aus',\n",
       "  'early',\n",
       "  'bird',\n",
       "  'oder',\n",
       "  'eule',\n",
       "  'flexible',\n",
       "  'arbeitszeiten',\n",
       "  'erlauben',\n",
       "  'es',\n",
       "  'dir',\n",
       "  'deine',\n",
       "  'arbeitszeit',\n",
       "  'individuell',\n",
       "  'zu',\n",
       "  'planen'],\n",
       " ['regelmäßige',\n",
       "  'learning',\n",
       "  'sessions',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'team',\n",
       "  'und',\n",
       "  'branchenexperten',\n",
       "  'sichern',\n",
       "  'dir',\n",
       "  'eine',\n",
       "  'steile',\n",
       "  'lernkurve',\n",
       "  'und',\n",
       "  'stetige',\n",
       "  'weiterentwicklung'],\n",
       " ['smoothes', 'onboarding', 'mit', 'unserem', 'buddy', 'system'],\n",
       " ['neben',\n",
       "  'dem',\n",
       "  'obligatorischen',\n",
       "  'müsli',\n",
       "  'haben',\n",
       "  'wir',\n",
       "  'eine',\n",
       "  'pastapesto',\n",
       "  'flatrate',\n",
       "  'im',\n",
       "  'büro'],\n",
       " ['startupmentalität', 'findest', 'du', 'bei', 'uns', 'natürlich', 'auch'],\n",
       " ['aber',\n",
       "  'mit',\n",
       "  'der',\n",
       "  'nötigen',\n",
       "  'vision',\n",
       "  'und',\n",
       "  'professionalität',\n",
       "  'um',\n",
       "  'zum',\n",
       "  'marktführer',\n",
       "  'unter',\n",
       "  'den',\n",
       "  'beratungsunternehmen',\n",
       "  'europa',\n",
       "  'zu',\n",
       "  'werden',\n",
       "  'die',\n",
       "  'sich',\n",
       "  'auf',\n",
       "  'mobile',\n",
       "  'marketing',\n",
       "  'spezialisiert',\n",
       "  'haben'],\n",
       " ['gute',\n",
       "  'performance',\n",
       "  'und',\n",
       "  'drive',\n",
       "  'werden',\n",
       "  'bei',\n",
       "  'uns',\n",
       "  'belohnt',\n",
       "  'und',\n",
       "  'führen',\n",
       "  'zu',\n",
       "  'kompetitiven',\n",
       "  'gehältern',\n",
       "  'über',\n",
       "  'dem',\n",
       "  'branchendurchschnitt',\n",
       "  'ein',\n",
       "  'motiviertes',\n",
       "  'team',\n",
       "  'mit',\n",
       "  'flachen',\n",
       "  'hierarchien',\n",
       "  'regelmäßig',\n",
       "  'stattfindende',\n",
       "  'team',\n",
       "  'events',\n",
       "  'und',\n",
       "  'company',\n",
       "  'offsites'],\n",
       " ['nimm',\n",
       "  'unseren',\n",
       "  'meetups',\n",
       "  'teil',\n",
       "  'und',\n",
       "  'vernetze',\n",
       "  'dich',\n",
       "  'mit',\n",
       "  'berlins',\n",
       "  'mobiler',\n",
       "  'marketing',\n",
       "  'startup',\n",
       "  'szene'],\n",
       " ['junior',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'fmx',\n",
       "  'role',\n",
       "  'junior',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'responsible',\n",
       "  'improving',\n",
       "  'maintaining',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'solutions',\n",
       "  'developing',\n",
       "  'implementing',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'solutions',\n",
       "  'clients'],\n",
       " ['working',\n",
       "  'within',\n",
       "  'business',\n",
       "  'intelligence',\n",
       "  'team',\n",
       "  'data',\n",
       "  'bi',\n",
       "  'analyst',\n",
       "  'big',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'act',\n",
       "  'point',\n",
       "  'contact',\n",
       "  'clients',\n",
       "  'projects',\n",
       "  'evolved'],\n",
       " ['upon',\n",
       "  'joining',\n",
       "  'get',\n",
       "  'know',\n",
       "  'customlytic',\n",
       "  '’',\n",
       "  'services',\n",
       "  'clients',\n",
       "  'getting',\n",
       "  'sense',\n",
       "  'headed',\n",
       "  'familiarise',\n",
       "  'quickly',\n",
       "  'expected',\n",
       "  'roll',\n",
       "  'sleeves',\n",
       "  'take',\n",
       "  'action'],\n",
       " ['addition',\n",
       "  'working',\n",
       "  'friendly',\n",
       "  'modern',\n",
       "  'work',\n",
       "  'atmosphere',\n",
       "  'right',\n",
       "  'prenzlauer',\n",
       "  'berg',\n",
       "  'one',\n",
       "  'liveliest',\n",
       "  'central',\n",
       "  'places',\n",
       "  'berlin'],\n",
       " ['responsibilities',\n",
       "  'support',\n",
       "  'area',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'enable',\n",
       "  'team',\n",
       "  'focus',\n",
       "  'delivering',\n",
       "  'insights',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'app',\n",
       "  'businesses',\n",
       "  'consult',\n",
       "  'teams',\n",
       "  'data',\n",
       "  'needs',\n",
       "  'serve',\n",
       "  'liaison',\n",
       "  'marketingproduct',\n",
       "  'teams',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'responsible',\n",
       "  'providing',\n",
       "  'insightful',\n",
       "  'data',\n",
       "  'deep',\n",
       "  'dives',\n",
       "  'analytical',\n",
       "  'assistance',\n",
       "  'clientfacing',\n",
       "  'business',\n",
       "  'development',\n",
       "  'accounting',\n",
       "  'teams',\n",
       "  'understand',\n",
       "  'challenges',\n",
       "  'clients',\n",
       "  'facing',\n",
       "  'today',\n",
       "  'tomorrow',\n",
       "  'analyzing',\n",
       "  'trends',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'data',\n",
       "  'providing',\n",
       "  'insight',\n",
       "  'explanation',\n",
       "  'improve',\n",
       "  'operations',\n",
       "  'run',\n",
       "  'ad',\n",
       "  'hoc',\n",
       "  'analysis',\n",
       "  'build',\n",
       "  'data',\n",
       "  'visualizations',\n",
       "  'continue',\n",
       "  'developing',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'infrastructure',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'help',\n",
       "  'foster',\n",
       "  'datadriven',\n",
       "  'culture',\n",
       "  'throughout',\n",
       "  'whole',\n",
       "  'team',\n",
       "  'apply',\n",
       "  'best',\n",
       "  'practices',\n",
       "  'profile',\n",
       "  'gained',\n",
       "  'first',\n",
       "  'experiences',\n",
       "  'data',\n",
       "  'bi',\n",
       "  'field',\n",
       "  'degree',\n",
       "  'related',\n",
       "  'field',\n",
       "  'like',\n",
       "  'information',\n",
       "  'systems',\n",
       "  'analytics',\n",
       "  'economics',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'statistics',\n",
       "  'another',\n",
       "  'similar',\n",
       "  'quantitative',\n",
       "  'field',\n",
       "  'techsavvy',\n",
       "  'enthusiastic',\n",
       "  'mobile',\n",
       "  'industry',\n",
       "  'fluent',\n",
       "  'english',\n",
       "  'working',\n",
       "  'englishspeaking',\n",
       "  'team',\n",
       "  'sound',\n",
       "  'good',\n",
       "  'great',\n",
       "  'analytical',\n",
       "  'skills',\n",
       "  'ability',\n",
       "  'make',\n",
       "  'reasonable',\n",
       "  'use',\n",
       "  'data',\n",
       "  'working',\n",
       "  'clients',\n",
       "  'first',\n",
       "  'knowledge',\n",
       "  'sql',\n",
       "  'python',\n",
       "  'familiar',\n",
       "  'git',\n",
       "  'basic',\n",
       "  'knowledge',\n",
       "  'redshift',\n",
       "  'andor',\n",
       "  'bigquery',\n",
       "  'plus',\n",
       "  'businessminded',\n",
       "  'passionate',\n",
       "  'diving',\n",
       "  'making',\n",
       "  'sense',\n",
       "  'large',\n",
       "  'diverse',\n",
       "  'datasets',\n",
       "  'bonus',\n",
       "  'points',\n",
       "  'agency',\n",
       "  'experience',\n",
       "  'german',\n",
       "  'language',\n",
       "  'skills',\n",
       "  'knowledge',\n",
       "  'reporting',\n",
       "  'tools',\n",
       "  'like',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'knowledge',\n",
       "  'google',\n",
       "  'cloud',\n",
       "  'platform',\n",
       "  'aws',\n",
       "  'ecosystem',\n",
       "  'offer',\n",
       "  'app',\n",
       "  'store',\n",
       "  'optimization',\n",
       "  'fullstack',\n",
       "  'consulting',\n",
       "  'agency',\n",
       "  'germany',\n",
       "  'advise',\n",
       "  'clients',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'product',\n",
       "  'management',\n",
       "  'analytics',\n",
       "  'topics'],\n",
       " ['bvg', 'job', 'ticket', 'ab', 'area'],\n",
       " ['highend',\n",
       "  'hardware',\n",
       "  'need',\n",
       "  'perfect',\n",
       "  'working',\n",
       "  'environment',\n",
       "  'office',\n",
       "  'remote'],\n",
       " ['unlimited',\n",
       "  'number',\n",
       "  'remote',\n",
       "  'workdays',\n",
       "  'whether',\n",
       "  'work',\n",
       "  'office',\n",
       "  'productive',\n",
       "  'somewhere',\n",
       "  'else',\n",
       "  'choose',\n",
       "  'early',\n",
       "  'bird',\n",
       "  'owl',\n",
       "  'flexible',\n",
       "  'working',\n",
       "  'hours',\n",
       "  'allow',\n",
       "  'plan',\n",
       "  'working',\n",
       "  'time',\n",
       "  'individually'],\n",
       " ['regular',\n",
       "  'learning',\n",
       "  'sessions',\n",
       "  'team',\n",
       "  'industry',\n",
       "  'experts',\n",
       "  'ensure',\n",
       "  'steep',\n",
       "  'learning',\n",
       "  'curve',\n",
       "  'continuous',\n",
       "  'development'],\n",
       " ['smooth', 'onboarding', 'buddy', 'system'],\n",
       " ['addition', 'obligatory', 'muesli', 'pastapesto', 'flat', 'rate', 'office'],\n",
       " ['course', 'also', 'find', 'startup', 'mentality', 'us'],\n",
       " ['necessary',\n",
       "  'vision',\n",
       "  'professionalism',\n",
       "  'become',\n",
       "  'market',\n",
       "  'leader',\n",
       "  'among',\n",
       "  'consultancies',\n",
       "  'europe',\n",
       "  'specializing',\n",
       "  'mobile',\n",
       "  'marketing'],\n",
       " ['good',\n",
       "  'performance',\n",
       "  'drive',\n",
       "  'rewarded',\n",
       "  'us',\n",
       "  'lead',\n",
       "  'competitive',\n",
       "  'salaries',\n",
       "  'industry',\n",
       "  'average'],\n",
       " ['motivated',\n",
       "  'team',\n",
       "  'flat',\n",
       "  'hierarchies',\n",
       "  'regular',\n",
       "  'team',\n",
       "  'events',\n",
       "  'company',\n",
       "  'offsites'],\n",
       " ['take',\n",
       "  'part',\n",
       "  'meetups',\n",
       "  'network',\n",
       "  'berlins',\n",
       "  'mobile',\n",
       "  'marketing',\n",
       "  'startup',\n",
       "  'scene'],\n",
       " ['vertragsdauer',\n",
       "  'monate',\n",
       "  'art',\n",
       "  'der',\n",
       "  'stelle',\n",
       "  'vollzeit',\n",
       "  'befristet',\n",
       "  'arbeitszeiten',\n",
       "  'keine',\n",
       "  'wochenenden',\n",
       "  'montag',\n",
       "  'bis',\n",
       "  'freitag',\n",
       "  'leistungen',\n",
       "  'betriebliche',\n",
       "  'altersvorsorge',\n",
       "  'betriebliche',\n",
       "  'weiterbildung',\n",
       "  'flexible',\n",
       "  'arbeitszeiten',\n",
       "  'homeoffice',\n",
       "  'kostenlose',\n",
       "  'getränke',\n",
       "  'kostenloses',\n",
       "  'oder',\n",
       "  'vergünstigtes',\n",
       "  'essen',\n",
       "  'berufserfahrung',\n",
       "  'aws',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'sql',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'homeoffice',\n",
       "  'ja',\n",
       "  'gerade',\n",
       "  'geschaltet',\n",
       "  'diesen',\n",
       "  'job',\n",
       "  'melden']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "understood-combat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "basic-lease",
   "metadata": {},
   "source": [
    "### Parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "stunning-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn df into list of sentences\n",
    "sentences = df[\"job_text_sent\"].tolist()\n",
    "\n",
    "### reduce the nesting of the list to fit the format of the Phrases module\n",
    "sentence = []\n",
    "for second in sentences:\n",
    "    for first in second:\n",
    "        sentence.append(first)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "neither-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the phraser to detect bi-grams\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "phrases = Phrases(sentence, min_count=30, progress_per=10000)\n",
    "### transform the list of sentences to detect bigrams\n",
    "sent = []\n",
    "for phrase in phrases[sentence]:\n",
    "    sent.append(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-thompson",
   "metadata": {},
   "source": [
    "### Word2vec model v2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "graphic-messenger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', 0.9418188333511353),\n",
       " ('java', 0.9352352023124695),\n",
       " ('programming_languages', 0.931969940662384),\n",
       " ('python_r', 0.923129677772522),\n",
       " ('kotlin', 0.9053773880004883),\n",
       " ('r', 0.8826804161071777),\n",
       " ('proficient', 0.880977988243103),\n",
       " ('coding', 0.869279682636261),\n",
       " ('javascript', 0.868744969367981),\n",
       " ('programming', 0.861573338508606)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model2 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model2.build_vocab(sent, progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model2.train(sent, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model2.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "stylish-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-135-2edb66decfc6>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  w2v_model2.most_similar([\"I\",\"want\"])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'I' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-2edb66decfc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"want\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \"\"\"\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'I' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v_model2.most_similar([\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-device",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "executive-badge",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    '''\n",
    "    Replace the text with the respective vectors if there are in the model vocabulary\n",
    "    '''\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            vector = w2v_model.wv.__getitem__(word)\n",
    "            new_text.append(vector)\n",
    "    \n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranging-provincial",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"vectorized_jobs\"] = df[\"job_text_tokenized\"].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-think",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quick test for Translation pipeline and saving code for posterity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "christian-persian",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'TD Reply is an innovation and marketing consultancy and part of the Reply Group . we are working on international data science projects for our clients such as Audi, Adidas, Coca-Cola, Miele, Telefonica, and BMW . you will collaborate with an experienced and enthusiastic'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### quick test for transformer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(df[\"job_text\"][10],min_length=20, max_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-riverside",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### vector extraction from transfomer model\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = pipeline(\"feature-extraction\", model = \"distilbert-base-cased\")\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    return  1 - spatial.distance.cosine(feature_extractor(s1)[0][-1], feature_extractor(s2)[0][-1])\n",
    "\n",
    "def get_features(s):\n",
    "    return feature_extractor(s)[0][-1]\n",
    "\n",
    "sentance1 = \"no one loves sushi\"\n",
    "sentance2 = \"I use java for backend stuff and I'm important\"\n",
    "sentance3 = \"I use html and css for be the frontend guy there is\"\n",
    "\n",
    "print(similarity(sentance1, sentance2))\n",
    "print(similarity(sentance2, sentance3))\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.cluster import KMeans\n",
    "model  = KMeans(n_clusters=2)\n",
    "X= np.array([get_features(sentance1),get_features(sentance2),get_features(sentance3)])\n",
    "model.fit(X)\n",
    "model.predict(X)\n",
    "tokens = s.lower().replace('  ',' ').replace('\\n',' ').split(' ')\n",
    "threshold = 0.8\n",
    "for token in tqdm(set(tokens)):\n",
    "    if threshold < similarity(token.lower(), 'Skills'.lower()):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "responsible-today",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accounting',\n",
       " 'marketing',\n",
       " 'sales',\n",
       " 'distribution',\n",
       " 'logistic',\n",
       " 'scm',\n",
       " 'hr',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'management',\n",
       " 'planning',\n",
       " 'strategy',\n",
       " 'strategic',\n",
       " 'strategical',\n",
       " 'bd',\n",
       " 'reporting',\n",
       " 'report',\n",
       " 'visualization',\n",
       " 'visualisation',\n",
       " 'kpi',\n",
       " 'business',\n",
       " 'stakeholder',\n",
       " 'client',\n",
       " 'industry',\n",
       " 'entrepreneurship',\n",
       " 'entrepreneur',\n",
       " 'entrepreneurial',\n",
       " 'consulting',\n",
       " 'consult',\n",
       " 'analyst',\n",
       " 'analyze',\n",
       " 'analyse',\n",
       " 'analysis',\n",
       " 'analytical',\n",
       " 'analytics',\n",
       " 'analytic',\n",
       " 'solution',\n",
       " 'olap',\n",
       " 'feasibility',\n",
       " 'measurable',\n",
       " 'profitable',\n",
       " 'commercial',\n",
       " 'crm',\n",
       " 'efficiency',\n",
       " 'advertising',\n",
       " 'managing',\n",
       " 'dashboards']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(\"../fydjob/data/dicts/skills_dict.json\") as json_file:\n",
    "    skills = json.load(json_file)\n",
    "\n",
    "skills[\"business\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "residential-immunology",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mbition',\n",
       " 'mercedes–benz',\n",
       " 'bringing',\n",
       " 'automotive',\n",
       " 'industry',\n",
       " 'future',\n",
       " 'working',\n",
       " 'softwaredefined',\n",
       " 'architecture',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'create',\n",
       " 'stateoftheart',\n",
       " 'customer',\n",
       " 'functions',\n",
       " 'span',\n",
       " 'one',\n",
       " 'domain',\n",
       " 'controllers',\n",
       " 'significant',\n",
       " 'processing',\n",
       " 'power']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"][10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "suited-mozambique",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alexanderherdt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [[(führende, NN), (app, NN), (marketing, NN), ...\n",
       "1       [[(student, NN), (mfx, NN), (work, NN), (colla...\n",
       "4       [[(phiture, NN), (growth, NN), (consultancy, N...\n",
       "5       [[(operating, NN)], [], [(desire, NN)], [], [(...\n",
       "6       [[(head, NN), (finance, NN), (home, NN), (miss...\n",
       "                              ...                        \n",
       "1841    [[(zalando, NN), (fulfillment, NN), (controlle...\n",
       "1846    [[(caya, NN)], [(manage, NN), (process, NN), (...\n",
       "1849    [[(position, NN), (point, NN), (contact, NN), ...\n",
       "1902    [[(position, NN), (point, NN), (contact, NN), ...\n",
       "1919    [[(industry, NN), (market, NN), (europe, NN), ...\n",
       "Name: job_text_sent, Length: 839, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def noun_extractor(text):\n",
    "    '''\n",
    "    This function takes a text and returns only the nouns in the text.\n",
    "    It can be adjusted to any POS by changing the tag below.\n",
    "    '''\n",
    "    new_text = []\n",
    "    for sentence in text:\n",
    "        tagged_sentence = pos_tag(sentence)\n",
    "        new_text.append([(word,tag) for word,tag in tagged_sentence if tag in (\"NN\")]) #change me!\n",
    "    return new_text\n",
    "    \n",
    "\n",
    "df[\"job_text_sent\"].apply(noun_extractor)   \n",
    "    \n",
    "            \n",
    "       \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-repository",
   "metadata": {},
   "source": [
    "## Word2vec trained exclusivley on nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-baking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

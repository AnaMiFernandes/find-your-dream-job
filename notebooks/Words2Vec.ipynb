{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proper-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metropolitan-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#from scipy import spatial\n",
    "import numpy as np\n",
    "#from transformers import pipeline, AutoTokenizer, TFPreTrainedModel  \n",
    "#from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "df = joblib.load(\"../fydjob/output/indeed_proc/ip_2021-03-05.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-calgary",
   "metadata": {},
   "source": [
    "## Baseline Words2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "frozen-andorra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 0.930187463760376),\n",
       " ('java', 0.9272656440734863),\n",
       " ('scala', 0.9162542819976807),\n",
       " ('programming', 0.906502366065979),\n",
       " ('proficient', 0.8983687162399292),\n",
       " ('kotlin', 0.8937568068504333),\n",
       " ('sql', 0.8867748975753784),\n",
       " ('coding', 0.8787362575531006),\n",
       " ('scripting', 0.8771183490753174),\n",
       " ('javascript', 0.8712866306304932)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### instanciate model\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(sentence, progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(sentence, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-affiliation",
   "metadata": {},
   "source": [
    "## Training a Words2vec model with bi-gram parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-matrix",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "'''\n",
    "Preprocessing for the Job descriptions in paresed in senteneces.\n",
    "Modified form the other preprocessing pipeline\n",
    "'''\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation_mod(text):\n",
    "    \n",
    "    punct = string.punctuation.replace(\".\",\"\")\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    for sent in text:\n",
    "        sent = word_tokenize(sent) \n",
    "        sent = [w for w in sent if w not in stop_words and w not in string.punctuation and w]  \n",
    "        new_text.append(sent)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-assembly",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-16aa97fcc5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### filter out german offers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tag_language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "### filter out german offers\n",
    "df = df[df[\"tag_language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply preprocessing to df\n",
    "df[\"job_text_sent\"]= df[\"job_text\"].apply(to_lower).apply(remove_number)\\\n",
    "                                    .apply(lambda x : x.replace('\\n',' '))\\\n",
    "                                    .apply(remove_punctuation_mod)\\\n",
    "                                    .apply(lambda x: sent_tokenize(x))\\\n",
    "                                    .apply(remove_stopwords)\\\n",
    "                                    .apply(lemmatize_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cosmetic-calvin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mbition',\n",
       "  'mercedesâ€“benz',\n",
       "  'bringing',\n",
       "  'automotive',\n",
       "  'industry',\n",
       "  'future',\n",
       "  'working',\n",
       "  'softwaredefined',\n",
       "  'architecture',\n",
       "  'allows',\n",
       "  'us',\n",
       "  'create',\n",
       "  'stateoftheart',\n",
       "  'customer',\n",
       "  'functions',\n",
       "  'span',\n",
       "  'one',\n",
       "  'domain',\n",
       "  'controllers',\n",
       "  'significant',\n",
       "  'processing',\n",
       "  'power'],\n",
       " ['one',\n",
       "  'highperformance',\n",
       "  'domain',\n",
       "  'controllers',\n",
       "  'used',\n",
       "  'automated',\n",
       "  'driving',\n",
       "  'functions',\n",
       "  'e.g'],\n",
       " ['automated', 'driving', 'highways'],\n",
       " ['berlin',\n",
       "  'sindelfingen',\n",
       "  'subsidiary',\n",
       "  'mercedesbenz',\n",
       "  'ag',\n",
       "  'perfecting',\n",
       "  'art',\n",
       "  'moving',\n",
       "  'people',\n",
       "  'products',\n",
       "  'services',\n",
       "  'b',\n",
       "  'years'],\n",
       " ['international',\n",
       "  'team',\n",
       "  'employees',\n",
       "  'want',\n",
       "  'integrate',\n",
       "  'people',\n",
       "  'appreciate',\n",
       "  'quality',\n",
       "  'creativity',\n",
       "  'teamwork',\n",
       "  'much'],\n",
       " ['together',\n",
       "  'create',\n",
       "  'passionate',\n",
       "  'technology',\n",
       "  'exciting',\n",
       "  'topics',\n",
       "  'like',\n",
       "  'automated',\n",
       "  'autonomous',\n",
       "  'driving',\n",
       "  'infotainment',\n",
       "  'systems',\n",
       "  'mobile',\n",
       "  'apps'],\n",
       " ['need',\n",
       "  'help',\n",
       "  'us',\n",
       "  'shape',\n",
       "  'future',\n",
       "  'role',\n",
       "  'audio',\n",
       "  'team',\n",
       "  'build',\n",
       "  'abstraction',\n",
       "  'layer',\n",
       "  'audio',\n",
       "  'hardware',\n",
       "  'applications'],\n",
       " ['goal',\n",
       "  'build',\n",
       "  'modern',\n",
       "  'c',\n",
       "  'audio',\n",
       "  'apis',\n",
       "  'easy',\n",
       "  'application',\n",
       "  'developers',\n",
       "  'integrate',\n",
       "  'hiding',\n",
       "  'away',\n",
       "  'complexity',\n",
       "  'imposed',\n",
       "  'strong',\n",
       "  'realtime',\n",
       "  'requirements',\n",
       "  'complex',\n",
       "  'audio',\n",
       "  'routing',\n",
       "  'policy',\n",
       "  'management',\n",
       "  'prioritization'],\n",
       " ['setup', 'coordinate', 'projects', 'visible', 'end', 'customer'],\n",
       " ['maintain',\n",
       "  'team',\n",
       "  'backlog',\n",
       "  'define',\n",
       "  'communicate',\n",
       "  'needs',\n",
       "  'team',\n",
       "  'define',\n",
       "  'prioritize',\n",
       "  'user',\n",
       "  'stories',\n",
       "  'cooperation',\n",
       "  'stakeholders',\n",
       "  'track',\n",
       "  'burn',\n",
       "  'bugs',\n",
       "  'tickets',\n",
       "  'approve',\n",
       "  'technical',\n",
       "  'documents',\n",
       "  'reports',\n",
       "  'ensure',\n",
       "  'time',\n",
       "  'quality',\n",
       "  'expectations',\n",
       "  'structured',\n",
       "  'communication',\n",
       "  'development',\n",
       "  'team',\n",
       "  'stakeholders',\n",
       "  'profile',\n",
       "  'education',\n",
       "  'degree',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'communication',\n",
       "  'technology',\n",
       "  'comparable',\n",
       "  'qualification',\n",
       "  'experience',\n",
       "  'product',\n",
       "  'owner',\n",
       "  'experience',\n",
       "  'building',\n",
       "  'audio',\n",
       "  'software',\n",
       "  'understanding',\n",
       "  'qa',\n",
       "  'test',\n",
       "  'processes',\n",
       "  'understanding',\n",
       "  'agile',\n",
       "  'classic',\n",
       "  'development',\n",
       "  'processes',\n",
       "  'tools',\n",
       "  'understanding',\n",
       "  'challenges',\n",
       "  'constraints',\n",
       "  'within',\n",
       "  'automotive',\n",
       "  'domain',\n",
       "  'processes',\n",
       "  'e.g'],\n",
       " ['automotive',\n",
       "  'spice',\n",
       "  'asil',\n",
       "  'data',\n",
       "  'protection',\n",
       "  'another',\n",
       "  'industry',\n",
       "  'orientation',\n",
       "  'communication',\n",
       "  'within',\n",
       "  'larger',\n",
       "  'organisations',\n",
       "  'least',\n",
       "  'one',\n",
       "  'complete',\n",
       "  'project',\n",
       "  'life',\n",
       "  'cycle',\n",
       "  'including',\n",
       "  'operations',\n",
       "  'nice',\n",
       "  'experience',\n",
       "  'pulseaudio',\n",
       "  'alsa',\n",
       "  'similar',\n",
       "  'audio',\n",
       "  'frameworks',\n",
       "  'experience',\n",
       "  'audio',\n",
       "  'signal',\n",
       "  'processing',\n",
       "  'algorithms',\n",
       "  'experience',\n",
       "  'embedded',\n",
       "  'linux',\n",
       "  'yocto',\n",
       "  'experience',\n",
       "  'invehicle',\n",
       "  'infotainment',\n",
       "  'systems',\n",
       "  'understanding',\n",
       "  'cc',\n",
       "  'andor',\n",
       "  'python',\n",
       "  'programming',\n",
       "  'languages',\n",
       "  'personal',\n",
       "  'skills',\n",
       "  'good',\n",
       "  'understanding',\n",
       "  'end',\n",
       "  'customer',\n",
       "  'needs',\n",
       "  'high',\n",
       "  'degree',\n",
       "  'flexibility',\n",
       "  'willingness',\n",
       "  'travel',\n",
       "  'well',\n",
       "  'developed',\n",
       "  'communication',\n",
       "  'team',\n",
       "  'work',\n",
       "  'skills',\n",
       "  'even',\n",
       "  'tough',\n",
       "  'situations',\n",
       "  'language',\n",
       "  'skills',\n",
       "  'proficient',\n",
       "  'english',\n",
       "  'german',\n",
       "  'plus',\n",
       "  'offer',\n",
       "  'chance',\n",
       "  'work',\n",
       "  'new',\n",
       "  'generation',\n",
       "  'infotainment',\n",
       "  'systems',\n",
       "  'power',\n",
       "  'millions',\n",
       "  'cars',\n",
       "  'international',\n",
       "  'interdisciplinary',\n",
       "  'innovation',\n",
       "  'lab',\n",
       "  'part',\n",
       "  'daimler',\n",
       "  'ag',\n",
       "  'great',\n",
       "  'company',\n",
       "  'values',\n",
       "  'passionate',\n",
       "  'live',\n",
       "  'every',\n",
       "  'day',\n",
       "  'work'],\n",
       " ['look',\n",
       "  'mbition.io',\n",
       "  'scroll',\n",
       "  'mbition',\n",
       "  'experience',\n",
       "  'agile',\n",
       "  'working',\n",
       "  'methods',\n",
       "  'open',\n",
       "  'feedback',\n",
       "  'culture',\n",
       "  'brand',\n",
       "  'new',\n",
       "  'modern',\n",
       "  'fully',\n",
       "  'accessible',\n",
       "  'office',\n",
       "  'facing',\n",
       "  'spree',\n",
       "  'flexible',\n",
       "  'working',\n",
       "  'hours',\n",
       "  'transportation',\n",
       "  'health',\n",
       "  'benefits',\n",
       "  'discounts',\n",
       "  'cars',\n",
       "  'free',\n",
       "  'coffee',\n",
       "  'fruits',\n",
       "  'interested',\n",
       "  'look',\n",
       "  'forward',\n",
       "  'receiving',\n",
       "  'complete',\n",
       "  'application',\n",
       "  'including',\n",
       "  'cv',\n",
       "  'english',\n",
       "  'german',\n",
       "  'relevant',\n",
       "  'references',\n",
       "  'following',\n",
       "  'information',\n",
       "  'job',\n",
       "  'title',\n",
       "  'reference',\n",
       "  'number',\n",
       "  'salary',\n",
       "  'expectations',\n",
       "  'earliest',\n",
       "  'start',\n",
       "  'date',\n",
       "  'would',\n",
       "  'like',\n",
       "  'encourage',\n",
       "  'people',\n",
       "  'health',\n",
       "  'impairments',\n",
       "  'apply',\n",
       "  'jobs',\n",
       "  'building',\n",
       "  'work',\n",
       "  'places',\n",
       "  'offer',\n",
       "  'possibilities',\n",
       "  'adjust',\n",
       "  'different',\n",
       "  'employee',\n",
       "  'requirements'],\n",
       " ['gerade',\n",
       "  'geschaltet',\n",
       "  'weiter',\n",
       "  'zur',\n",
       "  'bewerbung',\n",
       "  'diesen',\n",
       "  'job',\n",
       "  'melden']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "understood-combat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "basic-lease",
   "metadata": {},
   "source": [
    "### Parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stunning-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn df into list of sentences\n",
    "sentences = df[\"job_text_sent\"].tolist()\n",
    "\n",
    "### reduce the nesting of the list to fit the format of the Phrases module\n",
    "sentence = []\n",
    "for second in sentences:\n",
    "    for first in second:\n",
    "        sentence.append(first)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "neither-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the phraser to detect bi-grams\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "phrases = Phrases(sentence, min_count=30, progress_per=10000)\n",
    "### transform the list of sentences to detect bigrams\n",
    "sent = []\n",
    "for phrase in phrases[sentence]:\n",
    "    sent.append(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-thompson",
   "metadata": {},
   "source": [
    "### Word2vec model v2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "graphic-messenger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python_r', 0.9402478337287903),\n",
       " ('scala', 0.9381452202796936),\n",
       " ('programming_languages', 0.9356119632720947),\n",
       " ('java', 0.9177874326705933),\n",
       " ('kotlin', 0.8978232145309448),\n",
       " ('javascript', 0.8949280381202698),\n",
       " ('proficient', 0.8912734389305115),\n",
       " ('r', 0.871959388256073),\n",
       " ('programming', 0.8718017339706421),\n",
       " ('pyspark', 0.8647997379302979)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model2 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model2.build_vocab(sent, progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model2.train(sent, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model2.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stylish-faith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "global-device",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "executive-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    '''\n",
    "    Replace the text with the respective vectors if there are in the model vocabulary\n",
    "    '''\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            vector = w2v_model.wv.__getitem__(word)\n",
    "            new_text.append(vector)\n",
    "    \n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranging-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vectorized_jobs\"] = df[\"job_text_tokenized\"].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-think",
   "metadata": {},
   "source": [
    "## Quick test for Translation pipeline and saving code for posterity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "christian-persian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'MBition, Mercedes-Benz is bringing the automotive industry into the future . we are working on a software-defined architecture that allows us to create state-of-the-art customer functions . one of these high-performance domain controllers is used for automated driving functions (e.g. automated driving on highways) the company has been perfecting the art of moving people, products and services from A to B for more than 120 years . with an international team of 500 employees, we want to integrate people who appreciate quality,Â»Â»Â» .'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### quick test for transformer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(df[\"job_text\"][10],min_length=120, max_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vector extraction from transfomer model\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = pipeline(\"feature-extraction\", model = \"distilbert-base-cased\")\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    return  1 - spatial.distance.cosine(feature_extractor(s1)[0][-1], feature_extractor(s2)[0][-1])\n",
    "\n",
    "def get_features(s):\n",
    "    return feature_extractor(s)[0][-1]\n",
    "\n",
    "sentance1 = \"no one loves sushi\"\n",
    "sentance2 = \"I use java for backend stuff and I'm important\"\n",
    "sentance3 = \"I use html and css for be the frontend guy there is\"\n",
    "\n",
    "print(similarity(sentance1, sentance2))\n",
    "print(similarity(sentance2, sentance3))\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.cluster import KMeans\n",
    "model  = KMeans(n_clusters=2)\n",
    "X= np.array([get_features(sentance1),get_features(sentance2),get_features(sentance3)])\n",
    "model.fit(X)\n",
    "model.predict(X)\n",
    "tokens = s.lower().replace('  ',' ').replace('\\n',' ').split(' ')\n",
    "threshold = 0.8\n",
    "for token in tqdm(set(tokens)):\n",
    "    if threshold < similarity(token.lower(), 'Skills'.lower()):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-intellectual",
   "metadata": {},
   "source": [
    "## only recommending the skills that are in the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "biblical-diagnosis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'r',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'javascript',\n",
       " 'pyspark',\n",
       " 'sql',\n",
       " 'git',\n",
       " 'c',\n",
       " 'tensorflow']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar_skills(no_recom,dictionary,query):\n",
    "    '''\n",
    "    This function show the most similar skills to the input,which are also within the tech skills dictionary\n",
    "    '''\n",
    "    ### Turn the dictionary into a comprehensive list of all terms\n",
    "    term_list = []\n",
    "    for cat in dictionary.keys():\n",
    "        for word in dictionary[cat]:\n",
    "            term_list.append(word)\n",
    "\n",
    "   ### Fetch similar words from the w2v model\n",
    "    try:\n",
    "        model_skills = w2v_model.wv.most_similar(query,topn= 100)\n",
    "        \n",
    "    \n",
    "   ### only get the words without the distance measure \n",
    "        skill_words = []\n",
    "        for i in range(len(model_skills)):\n",
    "            skill_words.append(model_skills[i][0])\n",
    "    \n",
    "        return [skill for skill in skill_words if skill in term_list][0:no_recom] \n",
    "    except:\n",
    "        print(\"Sorry,word not found\")\n",
    "no_recom = 10\n",
    "\n",
    "with open(\"../fydjob/data/dicts/skills_dict.json\") as json_file:\n",
    "    dictionary= json.load(json_file)\n",
    "\n",
    "query = [\"python\"]\n",
    "\n",
    "\n",
    "most_similar_skills(no_recom,dictionary,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "judicial-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['business', 'knowledge', 'programming', 'soft_skills'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../fydjob/data/dicts/skills_dict.json\") as json_file:\n",
    "    dictionary= json.load(json_file)\n",
    "\n",
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moving-monster",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category_tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-61998116adaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"science\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcategory_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_similar_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_recom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'category_tagger' is not defined"
     ]
    }
   ],
   "source": [
    "no_recom = 10\n",
    "\n",
    "with open(\"../fydjob/data/dicts/skills_dict.json\") as json_file:\n",
    "    dictionary= json.load(json_file)\n",
    "\n",
    "query = [\"science\"]\n",
    "\n",
    "category_tagger(most_similar_skills(no_recom,dictionary,query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-cloud",
   "metadata": {},
   "source": [
    "## Tagging the skills/term with the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "perfect-ocean",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'skills' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bb8c12a70911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusiness_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"business\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mknowledge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"knowledge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprogramming_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"programming\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msoft_skills_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"soft_skills\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtech_adjectives_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tech_adjectives\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'skills' is not defined"
     ]
    }
   ],
   "source": [
    "business_dict = skills[\"business\"]\n",
    "knowledge_dict = skills[\"knowledge\"]\n",
    "programming_dict = skills[\"programming\"]\n",
    "soft_skills_dict = skills[\"soft_skills\"]\n",
    "\n",
    "\n",
    "    \n",
    "def category_tagger(x):\n",
    "    '''\n",
    "    This function assigns the respective category to the recommended list\n",
    "    '''\n",
    "    if x in business_dict:\n",
    "        return (x,\"business\")\n",
    "    if x in knowledge_dict:\n",
    "        return (x,\"knowledge\")\n",
    "    if x in programming_dict:\n",
    "        return (x,\"programming\")\n",
    "    if x in soft_skills_dict:\n",
    "        return (x,\"soft_skills\")\n",
    "    if x in tech_adjectives_dict:\n",
    "        return (x,\"tech_adjectives\")\n",
    "\n",
    "\n",
    "[category_tagger(i) for i in test_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "suited-mozambique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'job_text_sent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'job_text_sent'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-5f1bbd82cc48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job_text_sent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job_text_sent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'job_text_sent'"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def noun_extractor(text,pos):\n",
    "    '''\n",
    "    This function takes a text and returns only the nouns in the text.\n",
    "    It can be adjusted to any POS by changing the tag below.\n",
    "    '''\n",
    "    new_text = []\n",
    "    for sentence in text:\n",
    "        tagged_sentence = pos_tag(sentence)\n",
    "        new_text.append([word for word,tag in tagged_sentence if tag in (pos)]) #change me!\n",
    "    return new_text\n",
    "    \n",
    "pos = \"NN\"\n",
    "df[\"job_text_sent\"] =df[\"job_text_sent\"].apply(noun_extractor)\n",
    "              \n",
    "\n",
    "            \n",
    "       \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-repository",
   "metadata": {},
   "source": [
    "## Word2vec trained exclusivley on nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "behind-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract nouns\n",
    "\n",
    "df[\"job_text_sent\"] = df[\"job_text_sent\"].apply(noun_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "iraqi-allah",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-49923281aa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### building vocab with tokenized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job_text_sent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[0;32m--> 921\u001b[0;31m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0m\u001b[1;32m    922\u001b[0m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 )\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(df[\"job_text_sent\"], progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(df[\"job_text_sent\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "academic-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: fydjob is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e fydjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "activated-branch",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fydjob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ff0c275dd59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfydjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2VecPipline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fydjob'"
     ]
    }
   ],
   "source": [
    "from fydjob.Word2VecPipline import WordPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-hudson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

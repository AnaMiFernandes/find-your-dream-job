{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "metropolitan-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, TFPreTrainedModel  \n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "df = joblib.load(\"../raw_data/ip_2021-03-03.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-calgary",
   "metadata": {},
   "source": [
    "## Baseline Words2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "frozen-andorra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 0.9423992037773132),\n",
       " ('java', 0.9318829774856567),\n",
       " ('scala', 0.9018955826759338),\n",
       " ('pyspark', 0.8928499221801758),\n",
       " ('scripting', 0.8636457920074463),\n",
       " ('bash', 0.8596066236495972),\n",
       " ('proficient', 0.8571724891662598),\n",
       " ('sas', 0.8519904017448425),\n",
       " ('css', 0.8489394187927246),\n",
       " ('html', 0.8456547260284424)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### instanciate model\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(df[\"job_text_tokenized\"], progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(df[\"job_text_tokenized\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-affiliation",
   "metadata": {},
   "source": [
    "## Training a Words2vec model with bi-gram parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-matrix",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "radical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "'''\n",
    "Preprocessing for the Job descriptions in paresed in senteneces.\n",
    "Modified form the other preprocessing pipeline\n",
    "'''\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords_mod(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    new_text = []\n",
    "    for sentence in text:\n",
    "        new_sentence = tokenize(sentence)\n",
    "        new_sentence = [w for w in sentence if not w in stop_words and w] \n",
    "        new_text.append(new_sentence)\n",
    "  \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_punctuation_mod(text):\n",
    "    \n",
    "    punct = string.punctuation.replace(\".\",\"\")\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    for sent in text:\n",
    "        sent = word_tokenize(sent) \n",
    "        sent = [w for w in sent if w not in stop_words and w not in string.punctuation and w]  \n",
    "        new_text.append(sent)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "checked-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out german offers\n",
    "df = df[df[\"tag_language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fitting-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply preprocessing to df\n",
    "df[\"job_text_sent\"]= df[\"job_text\"].apply(to_lower).apply(remove_number)\\\n",
    "                                    .apply(lambda x : x.replace('\\n',' '))\\\n",
    "                                    .apply(remove_punctuation_mod)\\\n",
    "                                    .apply(lambda x: sent_tokenize(x))\\\n",
    "                                    .apply(remove_stopwords)\\\n",
    "                                    .apply(lemmatize_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "considerable-makeup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['responsible',\n",
       "  'improvement',\n",
       "  'taledo',\n",
       "  '’',\n",
       "  'search',\n",
       "  'matching',\n",
       "  'engine',\n",
       "  'candidates',\n",
       "  'jobs',\n",
       "  'business',\n",
       "  'drivers',\n",
       "  'data',\n",
       "  'science'],\n",
       " ['develop',\n",
       "  'compare',\n",
       "  'different',\n",
       "  'algorithmic',\n",
       "  'approaches',\n",
       "  'andor',\n",
       "  'ml',\n",
       "  'models'],\n",
       " ['monitor', 'production', 'performance', 'measure', 'success', 'work'],\n",
       " ['update', 'outdated', 'models'],\n",
       " ['research',\n",
       "  'discuss',\n",
       "  'algorithmical',\n",
       "  'well',\n",
       "  'model',\n",
       "  'improvements',\n",
       "  'regularly'],\n",
       " ['knowledgeable',\n",
       "  'developed',\n",
       "  'ai',\n",
       "  'community',\n",
       "  'propose',\n",
       "  '’',\n",
       "  'applicable',\n",
       "  'taledo'],\n",
       " ['expect', 'curious', 'nature'],\n",
       " ['like',\n",
       "  'solve',\n",
       "  'challenging',\n",
       "  'problems',\n",
       "  'proficient',\n",
       "  'python',\n",
       "  'worked',\n",
       "  'relevant',\n",
       "  'libraries',\n",
       "  'know',\n",
       "  'use',\n",
       "  'data',\n",
       "  'handling',\n",
       "  'numpy',\n",
       "  'pandas',\n",
       "  'dask',\n",
       "  'psycopg',\n",
       "  'mldl',\n",
       "  'scikitlearn',\n",
       "  'xgboost',\n",
       "  'keras',\n",
       "  'pytorch',\n",
       "  'spacy',\n",
       "  'visualization',\n",
       "  'seaborn',\n",
       "  'matplotlib',\n",
       "  'experience',\n",
       "  'evaluating',\n",
       "  'different',\n",
       "  'approaches',\n",
       "  'choosing',\n",
       "  'appropriate',\n",
       "  'metric',\n",
       "  'worked',\n",
       "  'search',\n",
       "  'matching',\n",
       "  'nlp',\n",
       "  'using',\n",
       "  'various',\n",
       "  'approaches',\n",
       "  'mldl',\n",
       "  'besides',\n",
       "  'success',\n",
       "  'stories',\n",
       "  'also',\n",
       "  'know',\n",
       "  '’',\n",
       "  'worked',\n",
       "  'experience',\n",
       "  'elasticsearch',\n",
       "  'plus',\n",
       "  'translate',\n",
       "  'user',\n",
       "  'talk',\n",
       "  'observed',\n",
       "  'behaviour',\n",
       "  'algorithmic',\n",
       "  'approach'],\n",
       " ['switch',\n",
       "  'fluently',\n",
       "  'levels',\n",
       "  'details',\n",
       "  'big',\n",
       "  'picture',\n",
       "  'algorithmical',\n",
       "  'details',\n",
       "  'creative',\n",
       "  'generate',\n",
       "  'ideas'],\n",
       " ['revolutionizing',\n",
       "  'industry',\n",
       "  'wed',\n",
       "  'love',\n",
       "  'see',\n",
       "  'exceptional',\n",
       "  'ideas',\n",
       "  '’',\n",
       "  'handson',\n",
       "  'like',\n",
       "  'get',\n",
       "  'things',\n",
       "  'done'],\n",
       " ['work',\n",
       "  'autonomously',\n",
       "  'proactive',\n",
       "  'speak',\n",
       "  'fluent',\n",
       "  'english',\n",
       "  'berlin',\n",
       "  'plus',\n",
       "  'offer',\n",
       "  'work',\n",
       "  'challenging',\n",
       "  'domain',\n",
       "  'matching',\n",
       "  'candidates',\n",
       "  'jobs',\n",
       "  'work',\n",
       "  'mission',\n",
       "  'real',\n",
       "  'impact',\n",
       "  'people',\n",
       "  'direct',\n",
       "  'impact',\n",
       "  'candidates',\n",
       "  'career',\n",
       "  'development',\n",
       "  'work',\n",
       "  'decides',\n",
       "  'put',\n",
       "  'cv'],\n",
       " ['furthermore', 'support', 'companies', 'important', 'asset', 'people'],\n",
       " ['responsibility',\n",
       "  'start',\n",
       "  'flat',\n",
       "  'hierarchy',\n",
       "  'environment',\n",
       "  '‘',\n",
       "  'doer',\n",
       "  'maker',\n",
       "  '’',\n",
       "  'mindset'],\n",
       " ['contribute',\n",
       "  'directly',\n",
       "  'taledos',\n",
       "  'growth',\n",
       "  'story',\n",
       "  'leave',\n",
       "  'footprint',\n",
       "  'trustful',\n",
       "  'environment',\n",
       "  'open',\n",
       "  'conversations',\n",
       "  'transparency',\n",
       "  'blame',\n",
       "  'shame',\n",
       "  'culture',\n",
       "  'team',\n",
       "  'personal',\n",
       "  'development',\n",
       "  'plan',\n",
       "  'discussed',\n",
       "  'manager',\n",
       "  'individually',\n",
       "  'frequent',\n",
       "  'feedback',\n",
       "  'manager',\n",
       "  'knowledge',\n",
       "  'sharing',\n",
       "  'constant',\n",
       "  'learning',\n",
       "  'e.g'],\n",
       " ['educational',\n",
       "  'budget',\n",
       "  'hackathons',\n",
       "  'flexible',\n",
       "  'working',\n",
       "  'hours',\n",
       "  'home',\n",
       "  'office'],\n",
       " ['maybe',\n",
       "  'attractive',\n",
       "  'corona',\n",
       "  'office',\n",
       "  'additional',\n",
       "  'vacation',\n",
       "  'days',\n",
       "  'based',\n",
       "  'tenure',\n",
       "  'seniority',\n",
       "  'international',\n",
       "  'team',\n",
       "  'team',\n",
       "  'events',\n",
       "  'socializing',\n",
       "  'activities',\n",
       "  'taledo',\n",
       "  'allinone',\n",
       "  'suite',\n",
       "  'talent',\n",
       "  'acquisition',\n",
       "  'relations'],\n",
       " ['offer',\n",
       "  'talent',\n",
       "  'marketing',\n",
       "  'recruiting',\n",
       "  'service',\n",
       "  'candidate',\n",
       "  'management',\n",
       "  'single',\n",
       "  'platform'],\n",
       " ['part',\n",
       "  'solution',\n",
       "  'matching',\n",
       "  'top',\n",
       "  'candidates',\n",
       "  'best',\n",
       "  'companies',\n",
       "  'germany',\n",
       "  'received',\n",
       "  'funding',\n",
       "  'eu'],\n",
       " ['besides',\n",
       "  'technology',\n",
       "  'team',\n",
       "  'recruiting',\n",
       "  'experts',\n",
       "  '’',\n",
       "  'lose',\n",
       "  'human',\n",
       "  'touch',\n",
       "  'oftentimes',\n",
       "  'missing',\n",
       "  'recruiting',\n",
       "  'solutions'],\n",
       " ['heute', 'weiter', 'zur', 'bewerbung', 'diesen', 'job', 'melden']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adopted-strike",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "basic-lease",
   "metadata": {},
   "source": [
    "### Parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "stunning-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn df into list of sentences\n",
    "sentences = df[\"job_text_sent\"].tolist()\n",
    "\n",
    "### reduce the nesting of the list to fit the format of the Phrases module\n",
    "sentence = []\n",
    "for second in sentences:\n",
    "    for first in second:\n",
    "        sentence.append(first)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "neither-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the phraser to detect bi-grams\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "phrases = Phrases(sentence, min_count=30, progress_per=10000)\n",
    "### transform the list of sentences to detect bigrams\n",
    "sent = []\n",
    "for phrase in phrases[sentence]:\n",
    "    sent.append(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-thompson",
   "metadata": {},
   "source": [
    "### Word2vec model v2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "graphic-messenger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', 0.9750685095787048),\n",
       " ('java', 0.9682307243347168),\n",
       " ('programming_languages', 0.966687798500061),\n",
       " ('r', 0.9658426642417908),\n",
       " ('programming', 0.9524619579315186),\n",
       " ('sql', 0.9322541952133179),\n",
       " ('javascript', 0.9288321733474731),\n",
       " ('proficiency', 0.9236559867858887),\n",
       " ('least_one', 0.9181228876113892),\n",
       " ('proficient', 0.9148669242858887)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model2 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model2.build_vocab(sent, progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model2.train(sent, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model2.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cleared-namibia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hands', 0.9449827075004578),\n",
       " ('programming_languages', 0.9349161386489868),\n",
       " ('similar', 0.9235848784446716),\n",
       " ('least_one', 0.923090934753418),\n",
       " ('software_engineering', 0.9210233688354492),\n",
       " ('programming', 0.9171253442764282),\n",
       " ('java', 0.9100304841995239),\n",
       " ('scala', 0.9072116613388062),\n",
       " ('solid', 0.9025342464447021),\n",
       " ('theoretical', 0.8921900391578674)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model2.wv.most_similar([\"science\",\"python\",\"r\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-device",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "executive-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    '''\n",
    "    Replace the text with the respective vectors if there are in the model vocabulary\n",
    "    '''\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            vector = w2v_model.wv.__getitem__(word)\n",
    "            new_text.append(vector)\n",
    "    \n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranging-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vectorized_jobs\"] = df[\"job_text_tokenized\"].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-think",
   "metadata": {},
   "source": [
    "## Quick test for Translation pipeline and saving code for posterity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "christian-persian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'TD Reply is an innovation and marketing consultancy and part of the Reply Group . we are working on international data science projects for our clients such as Audi, Adidas, Coca-Cola, Miele, Telefonica, and BMW . you will collaborate with an experienced and enthusiastic'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### quick test for transformer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(df[\"job_text\"][10],min_length=20, max_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vector extraction from transfomer model\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = pipeline(\"feature-extraction\", model = \"distilbert-base-cased\")\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    return  1 - spatial.distance.cosine(feature_extractor(s1)[0][-1], feature_extractor(s2)[0][-1])\n",
    "\n",
    "def get_features(s):\n",
    "    return feature_extractor(s)[0][-1]\n",
    "\n",
    "sentance1 = \"no one loves sushi\"\n",
    "sentance2 = \"I use java for backend stuff and I'm important\"\n",
    "sentance3 = \"I use html and css for be the frontend guy there is\"\n",
    "\n",
    "print(similarity(sentance1, sentance2))\n",
    "print(similarity(sentance2, sentance3))\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.cluster import KMeans\n",
    "model  = KMeans(n_clusters=2)\n",
    "X= np.array([get_features(sentance1),get_features(sentance2),get_features(sentance3)])\n",
    "model.fit(X)\n",
    "model.predict(X)\n",
    "tokens = s.lower().replace('  ',' ').replace('\\n',' ').split(' ')\n",
    "threshold = 0.8\n",
    "for token in tqdm(set(tokens)):\n",
    "    if threshold < similarity(token.lower(), 'Skills'.lower()):\n",
    "        print(token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

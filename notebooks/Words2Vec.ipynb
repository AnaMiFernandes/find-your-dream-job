{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metropolitan-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#from scipy import spatial\n",
    "import numpy as np\n",
    "#from transformers import pipeline, AutoTokenizer, TFPreTrainedModel  \n",
    "#from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "df = joblib.load(\"../fydjob/output/indeed_proc/ip_2021-03-05.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "accompanied-sussex",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_text</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_info</th>\n",
       "      <th>query_text</th>\n",
       "      <th>source</th>\n",
       "      <th>job_link</th>\n",
       "      <th>tag_language</th>\n",
       "      <th>reviews</th>\n",
       "      <th>job_info_tokenized</th>\n",
       "      <th>job_text_tokenized</th>\n",
       "      <th>job_text_tokenized_titlecase</th>\n",
       "      <th>job_title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Junior) Data Engineer (f/m/x)</td>\n",
       "      <td>Customlytics ist die führende App Marketing Be...</td>\n",
       "      <td>Customlytics GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>(Junior) Data Engineer (f/m/x)\\nCustomlytics G...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[junior, data, engineer, fmx, customlytics, gm...</td>\n",
       "      <td>[customlytics, ist, die, führende, app, market...</td>\n",
       "      <td>[Customlytics, ist, die, führende, App, Market...</td>\n",
       "      <td>[junior, data, engineer, fmx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Responsibilities\\n\\nAs working student (m/f/x)...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin]</td>\n",
       "      <td>[responsibilities, as, working, student, mfx, ...</td>\n",
       "      <td>[Responsibilities, As, working, student, mfx, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...</td>\n",
       "      <td>Aroundhome</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[aroundhome, bewertungen, berlin, teilzeit, pr...</td>\n",
       "      <td>[aufgaben, als, werkstudent, mwd, it, arbeites...</td>\n",
       "      <td>[Aufgaben, Als, Werkstudent, mwd, IT, arbeites...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Startdatum\\n\\nAb sofort\\n\\nDeine Aufgaben\\n\\nD...</td>\n",
       "      <td>Alexander Thamm GmbH</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Alexander Thamm GmbH - Berlin\\nFestanstellung</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[alexander, thamm, gmbh, berlin, festanstellung]</td>\n",
       "      <td>[startdatum, ab, sofort, deine, aufgaben, dein...</td>\n",
       "      <td>[Startdatum, Ab, sofort, Deine, Aufgaben, Dein...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full Stack Developer (m/f/d)</td>\n",
       "      <td>We’re Phiture: a leading mobile growth consult...</td>\n",
       "      <td>Phiture</td>\n",
       "      <td>BerlinKreuzberg</td>\n",
       "      <td>Full Stack Developer (m/f/d)\\nPhiture - Berlin...</td>\n",
       "      <td>data science</td>\n",
       "      <td>scrape_json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[full, stack, developer, mfd, phiture, berlink...</td>\n",
       "      <td>[were, phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[Were, Phiture, a, leading, mobile, growth, co...</td>\n",
       "      <td>[full, stack, developer, mfd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        job_title  \\\n",
       "0  (Junior) Data Engineer (f/m/x)   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4    Full Stack Developer (m/f/d)   \n",
       "\n",
       "                                            job_text               company  \\\n",
       "0  Customlytics ist die führende App Marketing Be...     Customlytics GmbH   \n",
       "1  Responsibilities\\n\\nAs working student (m/f/x)...            Aroundhome   \n",
       "2  Aufgaben\\nAls Werkstudent (m/w/d) IT arbeitest...            Aroundhome   \n",
       "3  Startdatum\\n\\nAb sofort\\n\\nDeine Aufgaben\\n\\nD...  Alexander Thamm GmbH   \n",
       "4  We’re Phiture: a leading mobile growth consult...               Phiture   \n",
       "\n",
       "          location                                           job_info  \\\n",
       "0           Berlin  (Junior) Data Engineer (f/m/x)\\nCustomlytics G...   \n",
       "1           Berlin                   Aroundhome6 Bewertungen - Berlin   \n",
       "2           Berlin  Aroundhome6 Bewertungen - Berlin\\nTeilzeit, Pr...   \n",
       "3           Berlin      Alexander Thamm GmbH - Berlin\\nFestanstellung   \n",
       "4  BerlinKreuzberg  Full Stack Developer (m/f/d)\\nPhiture - Berlin...   \n",
       "\n",
       "     query_text       source job_link tag_language  reviews  \\\n",
       "0  data science  scrape_json      NaN           en      NaN   \n",
       "1  data science  scrape_json      NaN           en      NaN   \n",
       "2  data science  scrape_json      NaN           de      NaN   \n",
       "3  data science  scrape_json      NaN           de      NaN   \n",
       "4  data science  scrape_json      NaN           en      NaN   \n",
       "\n",
       "                                  job_info_tokenized  \\\n",
       "0  [junior, data, engineer, fmx, customlytics, gm...   \n",
       "1                  [aroundhome, bewertungen, berlin]   \n",
       "2  [aroundhome, bewertungen, berlin, teilzeit, pr...   \n",
       "3   [alexander, thamm, gmbh, berlin, festanstellung]   \n",
       "4  [full, stack, developer, mfd, phiture, berlink...   \n",
       "\n",
       "                                  job_text_tokenized  \\\n",
       "0  [customlytics, ist, die, führende, app, market...   \n",
       "1  [responsibilities, as, working, student, mfx, ...   \n",
       "2  [aufgaben, als, werkstudent, mwd, it, arbeites...   \n",
       "3  [startdatum, ab, sofort, deine, aufgaben, dein...   \n",
       "4  [were, phiture, a, leading, mobile, growth, co...   \n",
       "\n",
       "                        job_text_tokenized_titlecase  \\\n",
       "0  [Customlytics, ist, die, führende, App, Market...   \n",
       "1  [Responsibilities, As, working, student, mfx, ...   \n",
       "2  [Aufgaben, Als, Werkstudent, mwd, IT, arbeites...   \n",
       "3  [Startdatum, Ab, sofort, Deine, Aufgaben, Dein...   \n",
       "4  [Were, Phiture, a, leading, mobile, growth, co...   \n",
       "\n",
       "             job_title_tokenized  \n",
       "0  [junior, data, engineer, fmx]  \n",
       "1                             []  \n",
       "2                             []  \n",
       "3                             []  \n",
       "4  [full, stack, developer, mfd]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-calgary",
   "metadata": {},
   "source": [
    "## Baseline Words2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frozen-andorra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 0.9612724781036377),\n",
       " ('scala', 0.9477375149726868),\n",
       " ('java', 0.9473602771759033),\n",
       " ('perl', 0.9447371363639832),\n",
       " ('matlab', 0.9395166635513306),\n",
       " ('c', 0.9001803398132324),\n",
       " ('julia', 0.899307906627655),\n",
       " ('cc', 0.8989156484603882),\n",
       " ('programing', 0.8947098255157471),\n",
       " ('javascala', 0.8921117782592773)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### instanciate model\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(df[\"job_text_tokenized\"], progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(df[\"job_text_tokenized\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brown-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"w2v_model_baseline.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-affiliation",
   "metadata": {},
   "source": [
    "## Training a Words2vec model with bi-gram parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-matrix",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "radical-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "'''\n",
    "Preprocessing for the Job descriptions in paresed in senteneces.\n",
    "Modified form the other preprocessing pipeline\n",
    "'''\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_number(text):\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation_mod(text):\n",
    "    \n",
    "    punct = string.punctuation.replace(\".\",\"\")\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_text = []\n",
    "    for sent in text:\n",
    "        sent = word_tokenize(sent) \n",
    "        sent = [w for w in sent if w not in stop_words and w not in string.punctuation and w]  \n",
    "        new_text.append(sent)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in text:\n",
    "        for word in sentence:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "        \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daily-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out german offers\n",
    "df = df[df[\"tag_language\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply preprocessing to df\n",
    "df[\"job_text_sent\"]= df[\"job_text\"].apply(to_lower).apply(remove_number)\\\n",
    "                                    .apply(lambda x : x.replace('\\n',' '))\\\n",
    "                                    .apply(remove_punctuation_mod)\\\n",
    "                                    .apply(lambda x: sent_tokenize(x))\\\n",
    "                                    .apply(remove_stopwords)\\\n",
    "                                    .apply(lemmatize_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cosmetic-calvin",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['customlytics',\n",
       "  'ist',\n",
       "  'die',\n",
       "  'führende',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'beratungsagentur',\n",
       "  'aus',\n",
       "  'berlin'],\n",
       " ['wir',\n",
       "  'bieten',\n",
       "  'consulting',\n",
       "  'und',\n",
       "  'handson',\n",
       "  'support',\n",
       "  'rund',\n",
       "  'um',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'strategie',\n",
       "  'produktmanagement',\n",
       "  'analytics',\n",
       "  'crm'],\n",
       " ['unser',\n",
       "  'team',\n",
       "  'erarbeitet',\n",
       "  'mit',\n",
       "  'unternehmen',\n",
       "  'jeder',\n",
       "  'größe',\n",
       "  'konzepte',\n",
       "  'zur',\n",
       "  'erfolgreichen',\n",
       "  'vermarktung',\n",
       "  'von',\n",
       "  'mobilen',\n",
       "  'apps'],\n",
       " ['dabei',\n",
       "  'decken',\n",
       "  'wir',\n",
       "  'nicht',\n",
       "  'nur',\n",
       "  'das',\n",
       "  'gesamte',\n",
       "  'spektrum',\n",
       "  'infrastruktureller',\n",
       "  'marketingthemen',\n",
       "  'ab'],\n",
       " ['wir',\n",
       "  'konzipieren',\n",
       "  'planen',\n",
       "  'und',\n",
       "  'steuern',\n",
       "  'sowohl',\n",
       "  'das',\n",
       "  'ui',\n",
       "  'ux',\n",
       "  'design',\n",
       "  'von',\n",
       "  'mobilen',\n",
       "  'apps',\n",
       "  'als',\n",
       "  'auch',\n",
       "  'performance',\n",
       "  'marketing',\n",
       "  'kampagnen',\n",
       "  'für',\n",
       "  'alle',\n",
       "  'app',\n",
       "  'verticals'],\n",
       " ['über', 'uns', 'unser', 'data', 'team', 'braucht', 'unterstützung'],\n",
       " ['du',\n",
       "  'bist',\n",
       "  'motiviert',\n",
       "  'und',\n",
       "  'von',\n",
       "  'der',\n",
       "  'mobile',\n",
       "  'industry',\n",
       "  'begeistert',\n",
       "  'dann',\n",
       "  'suchen',\n",
       "  'wir',\n",
       "  'dich',\n",
       "  'um',\n",
       "  'die',\n",
       "  'data',\n",
       "  'warehouselösungen',\n",
       "  'für',\n",
       "  'unsere',\n",
       "  'kunden',\n",
       "  'aus',\n",
       "  'der',\n",
       "  'app',\n",
       "  'industrie',\n",
       "  'zu',\n",
       "  'entwickeln',\n",
       "  'und',\n",
       "  'implementieren'],\n",
       " ['zusammen',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'biteam',\n",
       "  'arbeitest',\n",
       "  'du',\n",
       "  'kundenprojekten',\n",
       "  'der',\n",
       "  'entwicklung',\n",
       "  'implementierung',\n",
       "  'und',\n",
       "  'optimierung',\n",
       "  'von',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'projekten'],\n",
       " ['du',\n",
       "  'arbeitest',\n",
       "  'hierbei',\n",
       "  'eng',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'big',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'und',\n",
       "  'data',\n",
       "  'analyst',\n",
       "  'zusammen',\n",
       "  'der',\n",
       "  'entwicklung',\n",
       "  'von',\n",
       "  'dashboards',\n",
       "  'und',\n",
       "  'data',\n",
       "  'visualisierungen'],\n",
       " ['talent',\n",
       "  'technologieaffinität',\n",
       "  'und',\n",
       "  'eigenverantwortung',\n",
       "  'für',\n",
       "  'deine',\n",
       "  'arbeit',\n",
       "  'zu',\n",
       "  'übernehmen',\n",
       "  'sind',\n",
       "  'die',\n",
       "  'basis',\n",
       "  'unserer',\n",
       "  'teamkultur'],\n",
       " ['für',\n",
       "  'uns',\n",
       "  'ist',\n",
       "  'es',\n",
       "  'zweitrangig',\n",
       "  'ob',\n",
       "  'du',\n",
       "  'erst',\n",
       "  'anfang',\n",
       "  'deiner',\n",
       "  'beruflichen',\n",
       "  'laufbahn',\n",
       "  'stehst',\n",
       "  'oder',\n",
       "  'bereits',\n",
       "  'mehrjährige',\n",
       "  'berufserfahrung',\n",
       "  'hast',\n",
       "  'der',\n",
       "  'spirit',\n",
       "  'und',\n",
       "  'die',\n",
       "  'motivation',\n",
       "  'zählen'],\n",
       " ['deine',\n",
       "  'aufgaben',\n",
       "  'du',\n",
       "  'berätst',\n",
       "  'unsere',\n",
       "  'teams',\n",
       "  'hinsichtlich',\n",
       "  'der',\n",
       "  'datenanforderungen',\n",
       "  'und',\n",
       "  'bist',\n",
       "  'die',\n",
       "  'schnittstelle',\n",
       "  'zwischen',\n",
       "  'marketingprodukt',\n",
       "  'consultants',\n",
       "  'und',\n",
       "  'dem',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'bi',\n",
       "  'team'],\n",
       " ['du',\n",
       "  'unterstützt',\n",
       "  'bei',\n",
       "  'der',\n",
       "  'analyse',\n",
       "  'von',\n",
       "  'datenströmen',\n",
       "  'unserer',\n",
       "  'kunden',\n",
       "  'aus',\n",
       "  'der',\n",
       "  'mobilen',\n",
       "  'app',\n",
       "  'branche',\n",
       "  'dem',\n",
       "  'aufbereiten',\n",
       "  'von',\n",
       "  'daten',\n",
       "  'und',\n",
       "  'dem',\n",
       "  'entwickeln',\n",
       "  'von',\n",
       "  'datenmodellen'],\n",
       " ['du',\n",
       "  'unterstützt',\n",
       "  'unser',\n",
       "  'consultingteam',\n",
       "  'und',\n",
       "  'interne',\n",
       "  'abteilungen',\n",
       "  'als',\n",
       "  'experte',\n",
       "  'bei',\n",
       "  'der',\n",
       "  'vorverarbeitung',\n",
       "  'und',\n",
       "  'auswertung',\n",
       "  'großer',\n",
       "  'datenmengen'],\n",
       " ['du',\n",
       "  'versetzt',\n",
       "  'dich',\n",
       "  'die',\n",
       "  'lage',\n",
       "  'deiner',\n",
       "  'externen',\n",
       "  'und',\n",
       "  'internen',\n",
       "  'stakeholder',\n",
       "  'verstehst',\n",
       "  'ihre',\n",
       "  'ziele',\n",
       "  'und',\n",
       "  'anforderungen',\n",
       "  'und',\n",
       "  'übersetzt',\n",
       "  'diese',\n",
       "  'lösungen'],\n",
       " ['entwicklung',\n",
       "  'von',\n",
       "  'data',\n",
       "  'warehousingprodukten',\n",
       "  'die',\n",
       "  'unseren',\n",
       "  'wettbewerbsvorteil',\n",
       "  'langfristig',\n",
       "  'sichern',\n",
       "  'und',\n",
       "  'ausbauen'],\n",
       " ['dein',\n",
       "  'profil',\n",
       "  'du',\n",
       "  'arbeitet',\n",
       "  'gerne',\n",
       "  'auf',\n",
       "  'englisch',\n",
       "  'mit',\n",
       "  'internationalen',\n",
       "  'kolleginnen',\n",
       "  'du',\n",
       "  'hast',\n",
       "  'erste',\n",
       "  'erfahrung',\n",
       "  'im',\n",
       "  'data',\n",
       "  'oder',\n",
       "  'bo',\n",
       "  'bereich',\n",
       "  'und',\n",
       "  'hast',\n",
       "  'ein',\n",
       "  'abgeschlossenes',\n",
       "  'studium',\n",
       "  'im',\n",
       "  'bereich',\n",
       "  'information',\n",
       "  'systems',\n",
       "  'analytics',\n",
       "  'wirtschaftsinformatik',\n",
       "  'mathematik',\n",
       "  'oder',\n",
       "  'statistik',\n",
       "  'mit',\n",
       "  'affinität',\n",
       "  'zu',\n",
       "  'betriebswirtschaftlichen',\n",
       "  'themen',\n",
       "  'wie',\n",
       "  'z.b'],\n",
       " ['online', 'marketing'],\n",
       " ['du',\n",
       "  'begeisterst',\n",
       "  'dich',\n",
       "  'für',\n",
       "  'die',\n",
       "  'mobile',\n",
       "  'apptechnologiebranche',\n",
       "  'und',\n",
       "  'hast',\n",
       "  'großes',\n",
       "  'interesse',\n",
       "  'technischen',\n",
       "  'lösungen',\n",
       "  'und',\n",
       "  'wie',\n",
       "  'apps',\n",
       "  'unter',\n",
       "  'der',\n",
       "  'haube',\n",
       "  'funktionieren'],\n",
       " ['basiswissen', 'sql', 'python', 'und', 'git'],\n",
       " ['grundkenntnisse',\n",
       "  'redshift',\n",
       "  'undoder',\n",
       "  'bigquery',\n",
       "  'routinierter',\n",
       "  'umgang',\n",
       "  'mit',\n",
       "  'gängigen',\n",
       "  'anwendungen',\n",
       "  'wie',\n",
       "  'google',\n",
       "  'drive',\n",
       "  'microsoft',\n",
       "  'office',\n",
       "  'mac',\n",
       "  'os',\n",
       "  'und',\n",
       "  'keine',\n",
       "  'scheu',\n",
       "  'sich',\n",
       "  'schnell',\n",
       "  'verschiedene',\n",
       "  'projektmanagement',\n",
       "  'analyse',\n",
       "  'und',\n",
       "  'marketingtools',\n",
       "  'einzuarbeiten'],\n",
       " ['du',\n",
       "  'überzeugst',\n",
       "  'mit',\n",
       "  'proaktivität',\n",
       "  'und',\n",
       "  'lösungsorientierter',\n",
       "  'denkweise'],\n",
       " ['bonus',\n",
       "  'points',\n",
       "  'agentur',\n",
       "  'undoder',\n",
       "  'startup',\n",
       "  'erfahrung',\n",
       "  'erste',\n",
       "  'erfahrungen',\n",
       "  'mit',\n",
       "  'reporting',\n",
       "  'tools',\n",
       "  'wie',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'und',\n",
       "  'der',\n",
       "  'google',\n",
       "  'cloud',\n",
       "  'platform',\n",
       "  'aws',\n",
       "  'wir',\n",
       "  'bieten',\n",
       "  'mehr',\n",
       "  'als',\n",
       "  'app',\n",
       "  'store',\n",
       "  'optimization',\n",
       "  'wir',\n",
       "  'sind',\n",
       "  'die',\n",
       "  'einzige',\n",
       "  'fullstack',\n",
       "  'beratungsagentur',\n",
       "  'im',\n",
       "  'deutschen',\n",
       "  'raum',\n",
       "  'und',\n",
       "  'beraten',\n",
       "  'kunden',\n",
       "  'zu',\n",
       "  'vielfältigen',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'produktmanagement',\n",
       "  'und',\n",
       "  'analytics',\n",
       "  'themen'],\n",
       " ['bvg', 'jobticket', 'für', 'den', 'ab', 'bereich'],\n",
       " ['highend',\n",
       "  'hardware',\n",
       "  'die',\n",
       "  'du',\n",
       "  'für',\n",
       "  'deine',\n",
       "  'perfekte',\n",
       "  'arbeitsumgebung',\n",
       "  'im',\n",
       "  'büro',\n",
       "  'und',\n",
       "  'remote',\n",
       "  'benötigst'],\n",
       " ['frei',\n",
       "  'verfügbare',\n",
       "  'anzahl',\n",
       "  'von',\n",
       "  'remote',\n",
       "  'work',\n",
       "  'tagen',\n",
       "  'ob',\n",
       "  'du',\n",
       "  'im',\n",
       "  'büro',\n",
       "  'arbeitest',\n",
       "  'oder',\n",
       "  'woanders',\n",
       "  'produktiver',\n",
       "  'bist',\n",
       "  'es',\n",
       "  'dir',\n",
       "  'selbst',\n",
       "  'aus',\n",
       "  'early',\n",
       "  'bird',\n",
       "  'oder',\n",
       "  'eule',\n",
       "  'flexible',\n",
       "  'arbeitszeiten',\n",
       "  'erlauben',\n",
       "  'es',\n",
       "  'dir',\n",
       "  'deine',\n",
       "  'arbeitszeit',\n",
       "  'individuell',\n",
       "  'zu',\n",
       "  'planen'],\n",
       " ['regelmäßige',\n",
       "  'learning',\n",
       "  'sessions',\n",
       "  'mit',\n",
       "  'unserem',\n",
       "  'team',\n",
       "  'und',\n",
       "  'branchenexperten',\n",
       "  'sichern',\n",
       "  'dir',\n",
       "  'eine',\n",
       "  'steile',\n",
       "  'lernkurve',\n",
       "  'und',\n",
       "  'stetige',\n",
       "  'weiterentwicklung'],\n",
       " ['smoothes', 'onboarding', 'mit', 'unserem', 'buddy', 'system'],\n",
       " ['neben',\n",
       "  'dem',\n",
       "  'obligatorischen',\n",
       "  'müsli',\n",
       "  'haben',\n",
       "  'wir',\n",
       "  'eine',\n",
       "  'pastapesto',\n",
       "  'flatrate',\n",
       "  'im',\n",
       "  'büro'],\n",
       " ['startupmentalität', 'findest', 'du', 'bei', 'uns', 'natürlich', 'auch'],\n",
       " ['aber',\n",
       "  'mit',\n",
       "  'der',\n",
       "  'nötigen',\n",
       "  'vision',\n",
       "  'und',\n",
       "  'professionalität',\n",
       "  'um',\n",
       "  'zum',\n",
       "  'marktführer',\n",
       "  'unter',\n",
       "  'den',\n",
       "  'beratungsunternehmen',\n",
       "  'europa',\n",
       "  'zu',\n",
       "  'werden',\n",
       "  'die',\n",
       "  'sich',\n",
       "  'auf',\n",
       "  'mobile',\n",
       "  'marketing',\n",
       "  'spezialisiert',\n",
       "  'haben'],\n",
       " ['gute',\n",
       "  'performance',\n",
       "  'und',\n",
       "  'drive',\n",
       "  'werden',\n",
       "  'bei',\n",
       "  'uns',\n",
       "  'belohnt',\n",
       "  'und',\n",
       "  'führen',\n",
       "  'zu',\n",
       "  'kompetitiven',\n",
       "  'gehältern',\n",
       "  'über',\n",
       "  'dem',\n",
       "  'branchendurchschnitt',\n",
       "  'ein',\n",
       "  'motiviertes',\n",
       "  'team',\n",
       "  'mit',\n",
       "  'flachen',\n",
       "  'hierarchien',\n",
       "  'regelmäßig',\n",
       "  'stattfindende',\n",
       "  'team',\n",
       "  'events',\n",
       "  'und',\n",
       "  'company',\n",
       "  'offsites'],\n",
       " ['nimm',\n",
       "  'unseren',\n",
       "  'meetups',\n",
       "  'teil',\n",
       "  'und',\n",
       "  'vernetze',\n",
       "  'dich',\n",
       "  'mit',\n",
       "  'berlins',\n",
       "  'mobiler',\n",
       "  'marketing',\n",
       "  'startup',\n",
       "  'szene'],\n",
       " ['junior',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'fmx',\n",
       "  'role',\n",
       "  'junior',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'responsible',\n",
       "  'improving',\n",
       "  'maintaining',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'solutions',\n",
       "  'developing',\n",
       "  'implementing',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'solutions',\n",
       "  'clients'],\n",
       " ['working',\n",
       "  'within',\n",
       "  'business',\n",
       "  'intelligence',\n",
       "  'team',\n",
       "  'data',\n",
       "  'bi',\n",
       "  'analyst',\n",
       "  'big',\n",
       "  'data',\n",
       "  'engineer',\n",
       "  'act',\n",
       "  'point',\n",
       "  'contact',\n",
       "  'clients',\n",
       "  'projects',\n",
       "  'evolved'],\n",
       " ['upon',\n",
       "  'joining',\n",
       "  'get',\n",
       "  'know',\n",
       "  'customlytic',\n",
       "  '’',\n",
       "  'services',\n",
       "  'clients',\n",
       "  'getting',\n",
       "  'sense',\n",
       "  'headed',\n",
       "  'familiarise',\n",
       "  'quickly',\n",
       "  'expected',\n",
       "  'roll',\n",
       "  'sleeves',\n",
       "  'take',\n",
       "  'action'],\n",
       " ['addition',\n",
       "  'working',\n",
       "  'friendly',\n",
       "  'modern',\n",
       "  'work',\n",
       "  'atmosphere',\n",
       "  'right',\n",
       "  'prenzlauer',\n",
       "  'berg',\n",
       "  'one',\n",
       "  'liveliest',\n",
       "  'central',\n",
       "  'places',\n",
       "  'berlin'],\n",
       " ['responsibilities',\n",
       "  'support',\n",
       "  'area',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'enable',\n",
       "  'team',\n",
       "  'focus',\n",
       "  'delivering',\n",
       "  'insights',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'app',\n",
       "  'businesses',\n",
       "  'consult',\n",
       "  'teams',\n",
       "  'data',\n",
       "  'needs',\n",
       "  'serve',\n",
       "  'liaison',\n",
       "  'marketingproduct',\n",
       "  'teams',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'responsible',\n",
       "  'providing',\n",
       "  'insightful',\n",
       "  'data',\n",
       "  'deep',\n",
       "  'dives',\n",
       "  'analytical',\n",
       "  'assistance',\n",
       "  'clientfacing',\n",
       "  'business',\n",
       "  'development',\n",
       "  'accounting',\n",
       "  'teams',\n",
       "  'understand',\n",
       "  'challenges',\n",
       "  'clients',\n",
       "  'facing',\n",
       "  'today',\n",
       "  'tomorrow',\n",
       "  'analyzing',\n",
       "  'trends',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'data',\n",
       "  'providing',\n",
       "  'insight',\n",
       "  'explanation',\n",
       "  'improve',\n",
       "  'operations',\n",
       "  'run',\n",
       "  'ad',\n",
       "  'hoc',\n",
       "  'analysis',\n",
       "  'build',\n",
       "  'data',\n",
       "  'visualizations',\n",
       "  'continue',\n",
       "  'developing',\n",
       "  'clients',\n",
       "  '’',\n",
       "  'data',\n",
       "  'warehouse',\n",
       "  'infrastructure',\n",
       "  'data',\n",
       "  'engineering',\n",
       "  'team',\n",
       "  'help',\n",
       "  'foster',\n",
       "  'datadriven',\n",
       "  'culture',\n",
       "  'throughout',\n",
       "  'whole',\n",
       "  'team',\n",
       "  'apply',\n",
       "  'best',\n",
       "  'practices',\n",
       "  'profile',\n",
       "  'gained',\n",
       "  'first',\n",
       "  'experiences',\n",
       "  'data',\n",
       "  'bi',\n",
       "  'field',\n",
       "  'degree',\n",
       "  'related',\n",
       "  'field',\n",
       "  'like',\n",
       "  'information',\n",
       "  'systems',\n",
       "  'analytics',\n",
       "  'economics',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'statistics',\n",
       "  'another',\n",
       "  'similar',\n",
       "  'quantitative',\n",
       "  'field',\n",
       "  'techsavvy',\n",
       "  'enthusiastic',\n",
       "  'mobile',\n",
       "  'industry',\n",
       "  'fluent',\n",
       "  'english',\n",
       "  'working',\n",
       "  'englishspeaking',\n",
       "  'team',\n",
       "  'sound',\n",
       "  'good',\n",
       "  'great',\n",
       "  'analytical',\n",
       "  'skills',\n",
       "  'ability',\n",
       "  'make',\n",
       "  'reasonable',\n",
       "  'use',\n",
       "  'data',\n",
       "  'working',\n",
       "  'clients',\n",
       "  'first',\n",
       "  'knowledge',\n",
       "  'sql',\n",
       "  'python',\n",
       "  'familiar',\n",
       "  'git',\n",
       "  'basic',\n",
       "  'knowledge',\n",
       "  'redshift',\n",
       "  'andor',\n",
       "  'bigquery',\n",
       "  'plus',\n",
       "  'businessminded',\n",
       "  'passionate',\n",
       "  'diving',\n",
       "  'making',\n",
       "  'sense',\n",
       "  'large',\n",
       "  'diverse',\n",
       "  'datasets',\n",
       "  'bonus',\n",
       "  'points',\n",
       "  'agency',\n",
       "  'experience',\n",
       "  'german',\n",
       "  'language',\n",
       "  'skills',\n",
       "  'knowledge',\n",
       "  'reporting',\n",
       "  'tools',\n",
       "  'like',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'knowledge',\n",
       "  'google',\n",
       "  'cloud',\n",
       "  'platform',\n",
       "  'aws',\n",
       "  'ecosystem',\n",
       "  'offer',\n",
       "  'app',\n",
       "  'store',\n",
       "  'optimization',\n",
       "  'fullstack',\n",
       "  'consulting',\n",
       "  'agency',\n",
       "  'germany',\n",
       "  'advise',\n",
       "  'clients',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'app',\n",
       "  'marketing',\n",
       "  'product',\n",
       "  'management',\n",
       "  'analytics',\n",
       "  'topics'],\n",
       " ['bvg', 'job', 'ticket', 'ab', 'area'],\n",
       " ['highend',\n",
       "  'hardware',\n",
       "  'need',\n",
       "  'perfect',\n",
       "  'working',\n",
       "  'environment',\n",
       "  'office',\n",
       "  'remote'],\n",
       " ['unlimited',\n",
       "  'number',\n",
       "  'remote',\n",
       "  'workdays',\n",
       "  'whether',\n",
       "  'work',\n",
       "  'office',\n",
       "  'productive',\n",
       "  'somewhere',\n",
       "  'else',\n",
       "  'choose',\n",
       "  'early',\n",
       "  'bird',\n",
       "  'owl',\n",
       "  'flexible',\n",
       "  'working',\n",
       "  'hours',\n",
       "  'allow',\n",
       "  'plan',\n",
       "  'working',\n",
       "  'time',\n",
       "  'individually'],\n",
       " ['regular',\n",
       "  'learning',\n",
       "  'sessions',\n",
       "  'team',\n",
       "  'industry',\n",
       "  'experts',\n",
       "  'ensure',\n",
       "  'steep',\n",
       "  'learning',\n",
       "  'curve',\n",
       "  'continuous',\n",
       "  'development'],\n",
       " ['smooth', 'onboarding', 'buddy', 'system'],\n",
       " ['addition', 'obligatory', 'muesli', 'pastapesto', 'flat', 'rate', 'office'],\n",
       " ['course', 'also', 'find', 'startup', 'mentality', 'us'],\n",
       " ['necessary',\n",
       "  'vision',\n",
       "  'professionalism',\n",
       "  'become',\n",
       "  'market',\n",
       "  'leader',\n",
       "  'among',\n",
       "  'consultancies',\n",
       "  'europe',\n",
       "  'specializing',\n",
       "  'mobile',\n",
       "  'marketing'],\n",
       " ['good',\n",
       "  'performance',\n",
       "  'drive',\n",
       "  'rewarded',\n",
       "  'us',\n",
       "  'lead',\n",
       "  'competitive',\n",
       "  'salaries',\n",
       "  'industry',\n",
       "  'average'],\n",
       " ['motivated',\n",
       "  'team',\n",
       "  'flat',\n",
       "  'hierarchies',\n",
       "  'regular',\n",
       "  'team',\n",
       "  'events',\n",
       "  'company',\n",
       "  'offsites'],\n",
       " ['take',\n",
       "  'part',\n",
       "  'meetups',\n",
       "  'network',\n",
       "  'berlins',\n",
       "  'mobile',\n",
       "  'marketing',\n",
       "  'startup',\n",
       "  'scene'],\n",
       " ['vertragsdauer',\n",
       "  'monate',\n",
       "  'art',\n",
       "  'der',\n",
       "  'stelle',\n",
       "  'vollzeit',\n",
       "  'befristet',\n",
       "  'arbeitszeiten',\n",
       "  'keine',\n",
       "  'wochenenden',\n",
       "  'montag',\n",
       "  'bis',\n",
       "  'freitag',\n",
       "  'leistungen',\n",
       "  'betriebliche',\n",
       "  'altersvorsorge',\n",
       "  'betriebliche',\n",
       "  'weiterbildung',\n",
       "  'flexible',\n",
       "  'arbeitszeiten',\n",
       "  'homeoffice',\n",
       "  'kostenlose',\n",
       "  'getränke',\n",
       "  'kostenloses',\n",
       "  'oder',\n",
       "  'vergünstigtes',\n",
       "  'essen',\n",
       "  'berufserfahrung',\n",
       "  'aws',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'sql',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'google',\n",
       "  'data',\n",
       "  'studio',\n",
       "  'jahr',\n",
       "  'bevorzugt',\n",
       "  'homeoffice',\n",
       "  'ja',\n",
       "  'gerade',\n",
       "  'geschaltet',\n",
       "  'diesen',\n",
       "  'job',\n",
       "  'melden']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "understood-combat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "basic-lease",
   "metadata": {},
   "source": [
    "### Parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stunning-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn df into list of sentences\n",
    "sentences = df[\"job_text_sent\"].tolist()\n",
    "\n",
    "### reduce the nesting of the list to fit the format of the Phrases module\n",
    "sentence = []\n",
    "for second in sentences:\n",
    "    for first in second:\n",
    "        sentence.append(first)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neither-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the phraser to detect bi-grams\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "phrases = Phrases(sentence, min_count=30, progress_per=10000)\n",
    "### transform the list of sentences to detect bigrams\n",
    "sent = []\n",
    "for phrase in phrases[sentence]:\n",
    "    sent.append(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-thompson",
   "metadata": {},
   "source": [
    "### Word2vec model v2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "graphic-messenger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knowledge', 0.9957853555679321),\n",
       " ('spark', 0.9938962459564209),\n",
       " ('field', 0.9938292503356934),\n",
       " ('computer_science', 0.9924174547195435),\n",
       " ('cloud', 0.9908705949783325),\n",
       " ('engineering', 0.9908539056777954),\n",
       " ('infrastructure', 0.990630030632019),\n",
       " ('design', 0.9901478290557861),\n",
       " ('proficiency', 0.9899214506149292),\n",
       " ('java', 0.9878365993499756)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model2 = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model2.build_vocab(sent, progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model2.train(sent, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model2.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stylish-faith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "global-device",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "executive-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(text):\n",
    "    '''\n",
    "    Replace the text with the respective vectors if there are in the model vocabulary\n",
    "    '''\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            vector = w2v_model.wv.__getitem__(word)\n",
    "            new_text.append(vector)\n",
    "    \n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranging-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vectorized_jobs\"] = df[\"job_text_tokenized\"].apply(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-think",
   "metadata": {},
   "source": [
    "## Quick test for Translation pipeline and saving code for posterity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-persian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "### quick test for transformer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "summarizer(df[\"job_text\"][10],min_length=120, max_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "### vector extraction from transfomer model\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = pipeline(\"feature-extraction\", model = \"distilbert-base-cased\")\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    return  1 - spatial.distance.cosine(feature_extractor(s1)[0][-1], feature_extractor(s2)[0][-1])\n",
    "\n",
    "def get_features(s):\n",
    "    return feature_extractor(s)[0][-1]\n",
    "\n",
    "sentance1 = \"no one loves sushi\"\n",
    "sentance2 = \"I use java for backend stuff and I'm important\"\n",
    "sentance3 = \"I use html and css for be the frontend guy there is\"\n",
    "\n",
    "print(similarity(sentance1, sentance2))\n",
    "print(similarity(sentance2, sentance3))\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.cluster import KMeans\n",
    "model  = KMeans(n_clusters=2)\n",
    "X= np.array([get_features(sentance1),get_features(sentance2),get_features(sentance3)])\n",
    "model.fit(X)\n",
    "model.predict(X)\n",
    "tokens = s.lower().replace('  ',' ').replace('\\n',' ').split(' ')\n",
    "threshold = 0.8\n",
    "for token in tqdm(set(tokens)):\n",
    "    if threshold < similarity(token.lower(), 'Skills'.lower()):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "responsible-today",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accounting',\n",
       " 'marketing',\n",
       " 'sales',\n",
       " 'distribution',\n",
       " 'logistic',\n",
       " 'scm',\n",
       " 'hr',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'management',\n",
       " 'planning',\n",
       " 'strategy',\n",
       " 'strategic',\n",
       " 'strategical',\n",
       " 'bd',\n",
       " 'reporting',\n",
       " 'report',\n",
       " 'visualization',\n",
       " 'visualisation',\n",
       " 'kpi',\n",
       " 'business',\n",
       " 'stakeholder',\n",
       " 'client',\n",
       " 'industry',\n",
       " 'entrepreneurship',\n",
       " 'entrepreneur',\n",
       " 'entrepreneurial',\n",
       " 'consulting',\n",
       " 'consult',\n",
       " 'analyst',\n",
       " 'analyze',\n",
       " 'analyse',\n",
       " 'analysis',\n",
       " 'analytical',\n",
       " 'analytics',\n",
       " 'analytic',\n",
       " 'solution',\n",
       " 'olap',\n",
       " 'feasibility',\n",
       " 'measurable',\n",
       " 'profitable',\n",
       " 'commercial',\n",
       " 'crm',\n",
       " 'efficiency',\n",
       " 'advertising',\n",
       " 'managing',\n",
       " 'dashboards']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(\"../fydjob/data/dicts/skills_dict.json\") as json_file:\n",
    "    skills = json.load(json_file)\n",
    "\n",
    "skills[\"business\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "residential-immunology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[führende, app, marketing, beratungsagentur, ...\n",
       "1       [[student, mfx, work, collaboration, team, bus...\n",
       "4       [[phiture, growth, consultancy, closeknit, tea...\n",
       "5       [[operating], [], [desire], [], [passion], [pu...\n",
       "6       [[head, finance, home, mission, housing, every...\n",
       "                              ...                        \n",
       "1841    [[zalando, fulfillment, controller, perform], ...\n",
       "1846    [[caya], [manage, process, caya, platform], [c...\n",
       "1849    [[position, point, contact, architecture, soft...\n",
       "1902    [[position, point, contact, architecture, soft...\n",
       "1919    [[industry, market, road, freight, company, ge...\n",
       "Name: job_text_sent, Length: 839, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"job_text_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suited-mozambique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "def noun_extractor(text):\n",
    "    '''\n",
    "    This function takes a text and returns only the nouns in the text.\n",
    "    It can be adjusted to any POS by changing the tag below.\n",
    "    '''\n",
    "    new_text = []\n",
    "    for sentence in text:\n",
    "        tagged_sentence = pos_tag(sentence)\n",
    "        new_text.append([word for word,tag in tagged_sentence if tag in (\"NN\")]) #change me!\n",
    "    return new_text\n",
    "    \n",
    "\n",
    "df[\"job_text_sent\"] =df[\"job_text_sent\"].apply(noun_extractor)\n",
    "              \n",
    "\n",
    "            \n",
    "       \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-repository",
   "metadata": {},
   "source": [
    "## Word2vec trained exclusivley on nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "behind-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract nouns\n",
    "\n",
    "df[\"job_text_sent\"] = df[\"job_text_sent\"].apply(noun_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iraqi-allah",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-49923281aa34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m### building vocab with tokenized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job_text_sent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[0;32m--> 921\u001b[0;31m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0m\u001b[1;32m    922\u001b[0m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/fydjob/lib/python3.8/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 )\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1387\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1388\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     )\n",
    "\n",
    "### building vocab with tokenized words\n",
    "w2v_model.build_vocab(df[\"job_text_sent\"], progress_per=10000) \n",
    "\n",
    "\n",
    "###training the model on the dataset\n",
    "w2v_model.train(df[\"job_text_sent\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "\n",
    "### most similar words example\n",
    "w2v_model.wv.most_similar([\"python\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-branch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
